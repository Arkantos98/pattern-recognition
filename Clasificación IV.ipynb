{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de patrones: Clasificación\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "![ ](images/blank.png)\n",
    "![agents](images/binary_data_under_a_magnifying.jpg)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/pattern-recognition/blob/master/Clasificación%20IV.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de clasificación: Máquinas de vectores de soporte\n",
    "\n",
    "\n",
    "### El clasificador lineal\n",
    "\n",
    "Las SVM se basan en la idea de definir planos de decisión que separen a los objetos pertenecientes a diferentes clases. Para simplificar la discusión consideremos únicamente dos clases $A$ y $B$.\n",
    "\n",
    "![](images/classifier_plane.png)<br>\n",
    "\n",
    "El hiperplano que separa las clases $A$ y $B$ se define en el espacio descrito por los vectores de características $\\mathbf{x_i} =(x_{i,1},\\dots, x_{i,n})$. Para cualquier problema, existe un número infinito de esos planos. \n",
    "\n",
    "![](images/classifier_plane2.png)<br>\n",
    "\n",
    "El objetivo de clasificación es encontrar el hiperplano que separe mejor las regiones en el espacio de características ocupadas por cada una de las clases.\n",
    "\n",
    "Cualquiera de estos hiperplanos puede describirse como el conjunto de puntos ${\\mathbf {x}}$ para los cuales se cumple que\n",
    "\n",
    "￼$$\\mathbf {w}\\cdot \\mathbf {x} + b = 0$$\n",
    "\n",
    "siendo $\\mathbf {w}$ un vector normal al hiperplano y $b$ una constante tal que $$\\frac {b}{\\|{\\mathbf {w}}\\|}$$ es la distancia del hiperplano al origen a lo largo del vector $\\mathbf {w}$.\n",
    "\n",
    "Este hiperplano nos proporciona una regla de clasificación muy simple (*clasificador lineal*):\n",
    "\n",
    "* Si $\\mathbf {w} \\cdot \\mathbf x_{i} + b > 0$ entonces  $\\mathbf x_{i} \\in A$ (puntos por \"arriba\" del plano)<br><br>\n",
    "\n",
    "* Si $\\mathbf {w} \\cdot \\mathbf x_{i} + b < 0$ entonces  $\\mathbf x_{i} \\in B$ (puntos por \"abajo\" del plano)\n",
    "\n",
    "O bien, si asociamos la clase $A$ con el valor $y^* = 1$ y la clase $B$ con el valor $y^* = -1$, entonces podemos reescribir más convenientemente \n",
    "\n",
    "* $\\mathbf {w} \\cdot \\mathbf x_{i} + b > 0$ si  $y^*_i = 1$ <br><br>\n",
    "\n",
    "* $\\mathbf {w} \\cdot \\mathbf x_{i} + b < 0$ si  $y^*_i = -1$ \n",
    "\n",
    "Por supuesto, para $\\mathbf {w} \\cdot \\mathbf x_{i} + b = 0$, no hay decisión; son los puntos en la frontera.\n",
    "\n",
    "\n",
    "### Los hiperplanos de margen máximo y las SVM\n",
    "\n",
    "La estrategia de las SVM es elegir como mejor hiperplano, aquel que maximiza la distancia hacia los puntos de entrenamiento. Se define entonces, una región llamada de **margen máximo** alrededor de este **hiperplano óptimo** que representa una *zona de seguridad*: El clasificador se construye tratando de que los datos de entrenamiento queden ubicados no sólo del lado correcto del hiperplano óptimo, sino incluso fuera del margen máximo. Si posteriormente un nuevo dato cae dentro de esa zona, pero aún del lado correcto del hiperplano óptimo, aún sigue estando bien clasificado. El margen máximo es la región delimitada por los vectores de entrenamiento más cercanos al hiperplano y que son los llamados **vectores de soporte**, de donde toma el nombre la técnica.\n",
    "\n",
    "![](images/classifier_plane3.png)<br>\n",
    "\n",
    "Los datos se clasifican sobre la base de los dos hiperplanos definidos por los vectores de soporte; el *hiperplano positivo* para $y^*_i = 1$ (clase $A$) y el *hiperplano negativo* para $y^*_i = -1$ (clase $B$). Estos hiperplanos, entonces, pueden describirse mediante las ecuaciones\n",
    "\n",
    "* $\\mathbf {w} \\cdot \\mathbf {x} + b = 1$, y<br><br>\n",
    "\n",
    "* $\\mathbf {w} \\cdot \\mathbf {x} + b = -1$.\n",
    "\n",
    "La distancia entre estos dos hiperplanos es $\\frac {2}{\\|{\\mathbf {w}}\\|}$ de manera que, para maximizar la distancia, hay que minimizar $\\|\\mathbf {w}\\|$ (o $\\|{\\mathbf {w}}\\|^2$), con la restricción de que cada dato se quede dentro de su margen, es decir\n",
    "\n",
    "* $y^*_i(\\mathbf {w} \\cdot \\mathbf {x}_i + b) \\geq 1 \\quad \\forall\\quad  1\\leq i\\leq n$, siendo $n$ el número de datos de entrenamiento y $y^*_i\\in[-1, 1]$.\n",
    "\n",
    "El hiperplano óptimo se obtiene entonces minimizando la función $L(\\mathbf {w}, b)$ con restricciones adicionales:\n",
    "\n",
    "$$\n",
    "\\underset{\\large \\mathbf {w},\\ b}{\\min} L(\\mathbf {w}, b) = \\underset{\\large \\mathbf {w},\\ b}{\\min} \\left( \\frac{1}{2}\\|{\\mathbf {w}}\\|^2 \\right) \\quad \\text{sujeto a}\\quad y^*_i(\\mathbf {w} \\cdot \\mathbf {x}_i + b) \\geq 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "Este es un problema de optimización cuadrática sujeta a restricciones lineales. \n",
    "\n",
    "### Clasificador SVM de márgenes suaves\n",
    "\n",
    "La formulación anterior para la construcción de un hiperplano óptimo se conoce como el *caso de márgenes estrictos*: se exige que **todos** los datos de entrenamiento quedan dispuestos del lado que corresponde a su clase. Sin embargo, en muchos casos reales, los datos están lejos de ser linealmente separables; en la imagen a) de la siguiente figura, por ejemplo, cualquier línea que trate de separar los datos de entrenamiento dejará, irremediablemente, vectores mal clasificados. \n",
    "\n",
    "![](images/classifier_plane4.png)\n",
    "\n",
    "En casos como el mencionado antes existen dos opciones: a) utilizar otro tipo de función (computacionalmente más costosa) o b) \"*flexibilizar*\" las restricciones de clasificación, permitiendo al clasificador generar hiperplanos que dejen mal clasificados a un pequeño conjunto de datos. Esta estrategia, que permite seguir utilizando el clasificador lineal, es útil incluso en casos linealmente separables para aumentar el margen y con ello mejorar la generalización.\n",
    "\n",
    "Para suavizar las restricciones reformulamos el problema de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\underset{\\large \\mathbf {w},\\ b,\\ \\xi}{\\min} L(\\mathbf {w}, b, \\xi) = \\underset{\\large \\mathbf {w},\\ b,\\ \\xi}{\\min} \\left( \\frac{1}{2}\\|{\\mathbf {w}}\\|^2 + C\\sum_{i=1}^n \\xi_i \\right)\n",
    "\\quad \\text{sujeto a}\\quad y^*_i(\\mathbf {w} \\cdot \\mathbf {x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "\n",
    "Aquí se introducen dos elementos: los parámetros adicionales de optimización $\\xi_i$, llamados **variables de holgura** (*slack variables*) y la constante de penalización $C$. Los términos $C\\xi_i$ representan costos de penalización (al hiperplano candidato) por cada punto $\\mathbf {x}_i$ que quede fuera de zona. Si $0<\\xi_i\\leq1$ el vector quedó bien clasificado, pero dentro de la zona de margen; si $\\xi_i>1$, el vector queda mal clasificado. \n",
    "\n",
    "La constante $C>0$ es un parámetro de ajuste del método y representa qué tan estricta es el entrenamiento. Si $C$ es cercana a cero (penalización baja), entonces el entrenamiento es muy blando; esto significa que se tiene la libertad de crear una *zona de seguridad* amplia, aunque con muchos datos de entrenamiento que violan la restricción del margen. Si, por otra parte, $C\\to \\infty$, entonces el caso se aproxima al caso estricto, con pocos valores mal clasificados pero con una zona de seguridad pequeña, potencialmente debida a valores atípicos.\n",
    "\n",
    "<hr style=\"border-width: 2px;\">\n",
    "\n",
    "Las [máquinas de vectores de soporte](https://en.wikipedia.org/wiki/Support_vector_machine) (**SVM**) son una de las aproximaciones de clasificación más utilizados en análisis de textos.\n",
    "A continuación presentamos resultados de aplicación de máquina de vectores de soporte a la  clasificación de las revisiones de películas, utilizando diferentes valores de penalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display\n",
    "\n",
    "os.chdir('Data sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>with all this stuff going down at the moment w...</td>\n",
       "      <td>[stuff, going, moment, mj, started, listening,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>the classic war of the worlds   by timothy hi...</td>\n",
       "      <td>[classic, war, worlds, timothy, hines, enterta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>the film starts with a manager  nicholas bell ...</td>\n",
       "      <td>[film, starts, manager, nicholas, bell, giving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>it must be assumed that those who praised this...</td>\n",
       "      <td>[must, assumed, praised, film, greatest, filme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>superbly trashy and wondrously unpretentious  ...</td>\n",
       "      <td>[superbly, trashy, wondrously, unpretentious, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8196_8</td>\n",
       "      <td>1</td>\n",
       "      <td>i dont know why people think this is such a ba...</td>\n",
       "      <td>[dont, know, people, think, bad, movie, got, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7166_2</td>\n",
       "      <td>0</td>\n",
       "      <td>this movie could have been very good  but come...</td>\n",
       "      <td>[movie, could, good, comes, way, short, cheesy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10633_1</td>\n",
       "      <td>0</td>\n",
       "      <td>i watched this video at a friend s house  i m ...</td>\n",
       "      <td>[watched, video, friend, house, glad, waste, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319_1</td>\n",
       "      <td>0</td>\n",
       "      <td>a friend of mine bought this film for     and ...</td>\n",
       "      <td>[friend, mine, bought, film, even, grossly, ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8713_10</td>\n",
       "      <td>1</td>\n",
       "      <td>this movie is full of references  like  mad ma...</td>\n",
       "      <td>[movie, full, references, like, mad, max, ii, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sentiment                                             review  \\\n",
       "0   5814_8          1  with all this stuff going down at the moment w...   \n",
       "1   2381_9          1   the classic war of the worlds   by timothy hi...   \n",
       "2   7759_3          0  the film starts with a manager  nicholas bell ...   \n",
       "3   3630_4          0  it must be assumed that those who praised this...   \n",
       "4   9495_8          1  superbly trashy and wondrously unpretentious  ...   \n",
       "5   8196_8          1  i dont know why people think this is such a ba...   \n",
       "6   7166_2          0  this movie could have been very good  but come...   \n",
       "7  10633_1          0  i watched this video at a friend s house  i m ...   \n",
       "8    319_1          0  a friend of mine bought this film for     and ...   \n",
       "9  8713_10          1  this movie is full of references  like  mad ma...   \n",
       "\n",
       "                                               words  \n",
       "0  [stuff, going, moment, mj, started, listening,...  \n",
       "1  [classic, war, worlds, timothy, hines, enterta...  \n",
       "2  [film, starts, manager, nicholas, bell, giving...  \n",
       "3  [must, assumed, praised, film, greatest, filme...  \n",
       "4  [superbly, trashy, wondrously, unpretentious, ...  \n",
       "5  [dont, know, people, think, bad, movie, got, p...  \n",
       "6  [movie, could, good, comes, way, short, cheesy...  \n",
       "7  [watched, video, friend, house, glad, waste, m...  \n",
       "8  [friend, mine, bought, film, even, grossly, ov...  \n",
       "9  [movie, full, references, like, mad, max, ii, ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "movies_reviews = pd.read_csv(\"Movies Reviews/labeledTrainData.tsv\", sep='\\t')\n",
    "\n",
    "# Limpiar los documentos. Conservar sólo plabras (alfabéticas) y pasar a minúsculas\n",
    "movies_reviews.review = list(map(lambda row: re.sub(\"[^a-zA-Z]\", \" \", \n",
    "                                BeautifulSoup(row, \"lxml\").get_text().lower()), \n",
    "                                 movies_reviews.review))\n",
    "\n",
    "# Agregar una columna con la conversión de mensajes a listas de palabras\n",
    "# Se eliminan las palabras vacías\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "movies_reviews[\"words\"] = list(map(lambda row: [w for w in row.split() if not w in stops], \n",
    "                                   movies_reviews.review))\n",
    "display(movies_reviews.head(10))\n",
    "\n",
    "# Calculo del vector de carcterísticas, utilizano los 4000 términos más importantes\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features = 4000)\n",
    "X_data = vectorizer.fit_transform(movies_reviews.review)\n",
    "\n",
    "# Generar un arreglo con los valores de clasificación\n",
    "Sentiments = np.array([int(x) for x in movies_reviews.sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Sentiments, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Lineal, C=1 (default)\n",
      "Puntos mal clasificados (entrenamiento): 1182 de 20000 (5.91%)       \n",
      "Puntos mal clasificados (prueba): 646 de 5000 (12.92%)       \n",
      "Aciertos del 87.08%\n",
      "Tiempo: 0.16525006294250488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmLineal = LinearSVC(C=1)\n",
    "start_time = time.time()\n",
    "svmLineal.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Make an array of predictions on the test set\n",
    "preds_Lineal = svmLineal.predict(X_test)\n",
    "fails_Lineal = np.sum(y_test != preds_Lineal)\n",
    "preds_train_Lineal = svmLineal.predict(X_train)\n",
    "fails_train_Lineal = np.sum(y_train != preds_train_Lineal)\n",
    "print(\"SVM Lineal, C=1 (default)\\nPuntos mal clasificados (entrenamiento): {} de {} ({}%)\\\n",
    "       \\nPuntos mal clasificados (prueba): {} de {} ({}%)\\\n",
    "       \\nAciertos del {}%\\nTiempo: {}\\n\"\n",
    "      .format(fails_train_Lineal, len(y_train), 100*fails_train_Lineal/len(y_train),\n",
    "              fails_Lineal, len(y_test), 100*fails_Lineal/len(y_test), \n",
    "              svmLineal.score(X_test, y_test)*100, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra serie de corridas\n",
    "\n",
    "Corrida | Valor de $C$ | Puntos de entrenamiento mal clasificados | Puntos de prueba mal clasificados  \n",
    "----| ----| ------ | \n",
    "0   | 0.1 | 8.39% | 12.4%\n",
    "    | 0.5 | 6.49% | 13.0%\n",
    "    | 1.0 | 5.83% | 13.52%\n",
    "    | 100 | 3.47% | 17.98%\n",
    "    | 500 | 3.58% | 18.76%\n",
    "    | 1000 | 4.195% | 19.24%\n",
    "1   | 0.1 | 8.405% | 12.64%\n",
    "    | 0.5 | 6.56% | 13.04%\n",
    "    | 1.0 | 5.84% | 13.58%\n",
    "    | 100 | 3.415% | 17.7%\n",
    "    | 500 | 3.85% | 18.36%\n",
    "    | 1000 | 3.59% | 18.74%\n",
    "2   | 0.1 | 8.57% | 11.98%\n",
    "    | 0.5 | 6.685% | 11.9%\n",
    "    | 1.0 | 5.985% | 12.42%\n",
    "    | 100 | 3.92% | 16.94%\n",
    "    | 500 | 4.73% | 17.54%\n",
    "    | 1000 | 4.36% | 17.96%\n",
    "3   | 0.1 | 8.46% | 12.44%\n",
    "    | 0.5 | 6.48% | 12.72%\n",
    "    | 1.0 | 5.83% | 13.22%\n",
    "    | 100 | 3.42% | 17.96%\n",
    "    | 500 | 3.23% | 18.98%\n",
    "    | 1000 | 3.385% | 19.0%\n",
    "4   | 0.1 | 8.53% | 11.9%\n",
    "    | 0.5 | 6.55% | 12.58%\n",
    "    | 1.0 | 6.01% | 13.12%\n",
    "    | 100 | 3.615% | 16.74%\n",
    "    | 500 | 3.58% | 17.76%\n",
    "    | 1000 | 3.72% | 18.0%\n",
    "\n",
    "\n",
    "\n",
    "De acuerdo con las pruebas realizadas podemos observar que, según lo esperado, al incrementar el valor de $C$ disminuye el número de puntos de entrenamiento mal clasificados, en este caso, hasta llegar a $C=500$ (en las pruebas realizadas). \n",
    "\n",
    "<Font style=\"color:red\">**NOTA:**</Font> Es importante destacar que en realidad los únicos datos que conocemos son los de entrenamiento que, artificilmente dividimos en datos de \"entrenamiento\" y datos de prueba. Con estos datos conocidos (los de entrenamiento y los de prueba) esperamos entrenar el clasificador y estar preparados para datos nuevos, por ahora (realmente) **desconocidos**. De manera que reducir los errores en los datos de entrenamiento (en este caso al incrementar el valor de $C$) parece una buena decisión. \n",
    "\n",
    "El uso del clasificador sobre los datos de prueba, por otro lado, permiten observar que la reducción de errores de clasificación sobre los datos de entrenamiento, aumentando el valor de $C$, es decir, endureciendo las restricciones, tiene un costo alto en el nivel de generalización. \n",
    "\n",
    "Otro aspecto a observar en estos experimentos, es que para valores muy pequeños de $C$, el entrenamiento se ha vueto tan relajado que ya no hay aprendizaje.\n",
    "\n",
    "![](images/classifier_plane5.png)\n",
    "\n",
    "A continuación probaremos con otros datos conocidos. Los datos del Pima Indian Diabetes Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Pima Indian Data Set/pima-indians-diabetes.data\", \n",
    "                 names = ['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad', 'class'])\n",
    "\n",
    "df.loc[df['pad'] == 0,'pad'] = np.nan\n",
    "df.loc[df['ept'] == 0,'ept'] = np.nan\n",
    "df.loc[df['is2h'] == 0,'is2h'] = np.nan\n",
    "df.loc[df['imc'] == 0,'imc'] = np.nan\n",
    "df = df.dropna()\n",
    "\n",
    "df_pure = df[list(['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad'])]\n",
    "df_class = df[list(['class'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_trainPID, X_testPID, y_trainPID, y_testPID = train_test_split(\n",
    "    df_pure.values, df_class.values.ravel(), test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Lineal, C=1 (default)\n",
      "Puntos mal clasificados (entrenamiento): 86 de 314 (27.388535031847134%)       \n",
      "Puntos mal clasificados (prueba): 24 de 79 (30.379746835443036%)       \n",
      "Aciertos del 69.62025316455697%\n",
      "Tiempo: 0.011340856552124023\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmLineal = LinearSVC()\n",
    "start_time = time.time()\n",
    "svmLineal.fit(X_trainPID, y_trainPID)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "preds_train_Lineal = svmLineal.predict(X_trainPID)\n",
    "fails_train_Lineal = np.sum(y_trainPID != preds_train_Lineal)\n",
    "\n",
    "preds_Lineal = svmLineal.predict(X_testPID)\n",
    "fails_Lineal = np.sum(y_testPID != preds_Lineal)\n",
    "\n",
    "print(\"SVM Lineal, C=1 (default)\\nPuntos mal clasificados (entrenamiento): {} de {} ({}%)\\\n",
    "       \\nPuntos mal clasificados (prueba): {} de {} ({}%)\\\n",
    "       \\nAciertos del {}%\\nTiempo: {}\\n\"\n",
    "      .format(fails_train_Lineal, len(y_trainPID), 100*fails_train_Lineal/len(y_trainPID),\n",
    "              fails_Lineal, len(y_testPID), 100*fails_Lineal/len(y_testPID), \n",
    "              svmLineal.score(X_testPID, y_testPID)*100, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos obtenidos en 5 corridas arrojan lo siguiente:\n",
    "\n",
    "Corrida | Valor de $C$ | Puntos de entrenamiento mal clasificados | Puntos de prueba mal clasificados  \n",
    "----| ----| ------ | \n",
    "0   | 0.1 | 67.20% | 62.03%\n",
    "    | 0.5 | 32.48% | 34.18%\n",
    "    | 1.0 | 32.48% | 34.18%\n",
    "    | 100 | 32.48% | 35.44%\n",
    "    | 500 | 53.18% | 48.10%\n",
    "    | 1000 | 32.80% | 35.44%\n",
    "1   | 0.1 | 24.52% | 35.444%\n",
    "    | 0.5 | 31.84% | 30.38%\n",
    "    | 1.0 | 60.19% | 59.49%\n",
    "    | 100 | 33.44% | 31.65%\n",
    "    | 500 | 33.44% | 31.65%\n",
    "    | 1000 | 33.44% | 30.38%\n",
    "2   | 0.1 | 27.07% | 29.11%\n",
    "    | 0.5 | 46.82% | 51.90%\n",
    "    | 1.0 | 27.07% | 25.323%\n",
    "    | 100 | 34.08% | 26.58%\n",
    "    | 500 | 34.39% | 26.58%\n",
    "    | 1000 | 51.59% | 63.29%\n",
    "3   | 0.1 | 34.71% | 26.58%\n",
    "    | 0.5 | 33.76% | 26.58%\n",
    "    | 1.0 | 29.94% | 25.32%\n",
    "    | 100 | 30.25% | 17.72%\n",
    "    | 500 | 30.25% | 17.72%\n",
    "    | 1000 | 34.40% | 26.58%\n",
    "4   | 0.1 | 30.25% | 30.38%\n",
    "    | 0.5 | 33.44% | 31.65%\n",
    "    | 1.0 | 32.80% | 31.65%\n",
    "    | 100 | 65.29% | 68.35%\n",
    "    | 500 | 39.81% | 34.18%\n",
    "    | 1000 | 33.44% | 31.65%\n",
    "\n",
    "A partir de estos resultdos podemos observar que, en este caso, no hay un valor de $C$ claramente superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones kernel (núcleo) \n",
    "\n",
    "Las máquinas de vectores de soporte se basan en la idea de utilizar hiperplanos para separar, mediante artefactos lineales, un conjunto de datos en dos clases. Cuando tenemos clases no bien separadas, aún podemos utilizar las VSM para clasificar los datos, flexibilizando la construcción de los hiperplanos y aceptando, implícitamente, la existencia de algunos datos mal clasificados. Sin embargo, en muchos casos, los datos presentan una naturaleza inherentemente no lineales, por lo cual, el uso de hiperplanos para separar las clases no es una buena elección.\n",
    "\n",
    "![](images/classifier_plane6.png)\n",
    "\n",
    "Una alternativa, para utilizar las VSM en datos no lineales es transformar los datos mediante una transformación $\\phi : \\mathbb{R}^N \\to \\mathbb{R}^{N+d}$ tal que los datos \"equivalentes\" sean linealmente separables en el nuevo espacio $\\mathbb{R}^{N+d}$. En el caso anterior, por ejemplo, es posible separar los datos con un hiperplano si utilizamos la transformación $\\phi ([x, y]) = [x, y, x^2+y^2]$, donde $N=2$ y $d=1$. Analizando los datos en el espacio ampliado, con esta dimensión adicional, podemos claramente separar los puntos mediante un hiperplano, como se muestra en la figura:\n",
    "\n",
    "![](images/classifier_plane7.png)\n",
    "\n",
    "El nuevo hiperplano óptimo puede expresarse como \n",
    "\n",
    "￼$$\\mathbf {w}\\cdot \\phi(\\mathbf {x}) + b = 0$$\n",
    "\n",
    "Esta estrategia parece ser una buena solución, sin embargo, el costo computacional puede ser muy alto si crece el valor de $d$. Para problemas con un gran número de atributos el costo se vuelve intratable. \n",
    "\n",
    "Sin embargo, el clasificador no requiere trabajar directamente en el espacio aumentado, ni durante el entrenamiento ni durante la prueba. Puede demostrarse (mediante multiplicadores de Lagrange) que (para el hiperplano original)\n",
    "\n",
    "$$\\mathbf {w} = \\sum _{i=1}^n c_i y_i \\mathbf {x}_i$$\n",
    "\n",
    "donde los $c_i$ son constantes de optimización. Entonces, podemos reescribir\n",
    "\n",
    "￼$$\\sum _{i=1}^n c_i y_i \\left( \\phi(\\mathbf {x}_i) \\cdot \\phi(\\mathbf {x}) \\right) + b = 0$$\n",
    "\n",
    "\n",
    "De donde se desprende que lo que requermos es calcular los productos interiores modificados de los vectores de entrenamiento: \n",
    "\n",
    "$$\\phi(\\mathbf {x}_i) \\cdot \\phi(\\mathbf {x})$$\n",
    "\n",
    "\n",
    "Dado un espacio de características $\\mathcal {X}$, existen ciertas funciones $k\\colon \\mathcal {X}\\times \\mathcal {X}\\to \\mathbb {R} $ que, pueden expresarse como producto interior en otro espacio $\\mathcal {V}$. Este tipo de funciones se sele denominar como un *kernel* o *función kernel*.\n",
    "\n",
    "#### Tipos de funciones kernel\n",
    "**Funciones de base radial - RBF**. Una **función de base redial** (*radial basis function - RBF*) es una función real $\\phi (\\mathbf {x})$ cuyo valor para un punto dado $\\mathbf {x}$ depende únicamente de la distancia de $\\mathbf {x}$ al origen, de manera que $\\phi (\\mathbf {x})=\\phi (\\|\\mathbf {x} \\|)$ o, alternativamente, de la distancia de $\\mathbf {x}$ a otro punto cualquiera $\\mathbf {c}$, tal que $\\phi (\\mathbf {x}, \\mathbf {c} ) = \\phi (\\|\\mathbf {x} -\\mathbf {c} \\|)$. \n",
    "\n",
    "Una de estas funciones, utilizada muy frecuentemente como función kernel en diversos clasificadores, particularmente en máquinas de vectores de soporte, y llamada *función kernel de base radial* es:\n",
    "\n",
    "$$K(\\mathbf {x}, \\mathbf {x'} ) = \\exp(-\\gamma ||\\mathbf {x} -\\mathbf {x'} ||^{2})$$\n",
    "\n",
    "con $\\gamma =\\frac {1}{2\\sigma ^{2}}$.\n",
    "\n",
    "A continuación mostramos los resultados de varias corridas utilizando una máquina de vectores de soporte con función kernel RBF y diversos valores de penalización sobre los datos de revisiones de películas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RBF, C=1.0\n",
      "Puntos mal clasificados (entrenamiento): 9991 de 20000 (49.955%)       \n",
      "Puntos mal clasificados (prueba): 2509 de 5000 (50.18%)       \n",
      "Aciertos del 49.82%\n",
      "Tiempo: 189.208477973938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmRbf = SVC(kernel='rbf', C=1.0)\n",
    "start_time = time.time()\n",
    "svmRbf.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "preds_train_Rbf = svmRbf.predict(X_train)\n",
    "fails_train_Rbf = np.sum(y_train != preds_train_Rbf)\n",
    "\n",
    "preds_Rbf = svmRbf.predict(X_test)\n",
    "fails_Rbf = np.sum(y_test != preds_Rbf)\n",
    "\n",
    "print(\"SVM RBF, C=1.0\\nPuntos mal clasificados (entrenamiento): {} de {} ({}%)\\\n",
    "       \\nPuntos mal clasificados (prueba): {} de {} ({}%)\\\n",
    "       \\nAciertos del {}%\\nTiempo: {}\\n\"\n",
    "      .format(fails_train_Rbf, len(y_train), 100*fails_train_Rbf/len(y_train),\n",
    "              fails_Rbf, len(y_test), 100*fails_Rbf/len(y_test), \n",
    "              svmRbf.score(X_test, y_test)*100, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los experimentos obtenidos en 5 corridas con los datos de revisiones de películas arrojan lo siguiente:\n",
    "\n",
    "Corrida | Valor de $C$ | Error en entrenamiento mal clasificados | Puntos de prueba mal clasificados  \n",
    "----| ----| ------ | \n",
    "0   | 0.1 | 49.91% | 50.36%\n",
    "    | 0.5 | 49.91% | 50.36%\n",
    "    | 1.0 | 49.91% | 50.36%\n",
    "    | 100 | 13.06% | 13.94%\n",
    "    | 500 | 9.335% | 12.18%\n",
    "    | 1000 | 8.395% | 12.34%\n",
    "1   | 0.1 | 49.895% | 50.42%\n",
    "    | 0.5 | 49.895% | 50.42%\n",
    "    | 1.0 | 49.895% | 50.42%\n",
    "    | 100 | 12.7% | 14.54%\n",
    "    | 500 | 9.25% | 12.98%\n",
    "    | 1000 | 8.12% | 12.82%\n",
    "2   | 0.1 | 49.62% | 51.52%\n",
    "    | 0.5 | 49.62% | 51.52%\n",
    "    | 1.0 | 49.62% | 51.52%\n",
    "    | 100 | 12.825% | 14.82%\n",
    "    | 500 | 9.25% | 13.1%\n",
    "    | 1000 | 8.195% | 13.08%\n",
    "3   | 0.1 | 49.9% | 50.4%\n",
    "    | 0.5 | 49.9% | 50.4%\n",
    "    | 1.0 | 49.9% | 50.4%\n",
    "    | 100 | 12.875% | 13.9%\n",
    "    | 500 | 9.435% | 11.92%\n",
    "    | 1000 | 8.445% | 11.82%\n",
    "4   | 0.1 | 49.55% | 51.8%\n",
    "    | 0.5 | 49.55% | 51.8%\n",
    "    | 1.0 | 49.55% | 51.8%\n",
    "    | 100 | 12.57% | 14.98%\n",
    "    | 500 | 9.22% | 13.74%\n",
    "    | 1000 | 8.135% | 13.86%\n",
    "\n",
    "Observamos que ahora los mejores resultados se obtienen para valores de penalización de alrededor de 500. Los resultados parecen ser estables, sin embargo, la complejidad de la técnica es superior, por lo que no parece ser mejor elección que el clasificador lineal (que en este problema arroja resultados ya singularmente buenos).\n",
    "\n",
    "A continuación presentamos los resultados obtenidos al utilizar una SVM con función RBF sobre los datos de diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RBF, C=1.0\n",
      "Puntos mal clasificados (entrenamiento): 0 de 314 (0.0%)       \n",
      "Puntos mal clasificados (prueba): 30 de 79 (37.9746835443038%)       \n",
      "Aciertos del 62.0253164556962%\n",
      "Tiempo: 0.004086017608642578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmRbf = SVC(kernel='rbf')\n",
    "start_time = time.time()\n",
    "svmRbf.fit(X_trainPID, y_trainPID)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "preds_train_Rbf = svmRbf.predict(X_trainPID)\n",
    "fails_train_Rbf = np.sum(y_trainPID != preds_train_Rbf)\n",
    "\n",
    "preds_Rbf = svmRbf.predict(X_testPID)\n",
    "fails_Rbf = np.sum(y_testPID != preds_Rbf)\n",
    "\n",
    "print(\"SVM RBF, C=1.0\\nPuntos mal clasificados (entrenamiento): {} de {} ({}%)\\\n",
    "       \\nPuntos mal clasificados (prueba): {} de {} ({}%)\\\n",
    "       \\nAciertos del {}%\\nTiempo: {}\\n\"\n",
    "      .format(fails_train_Rbf, len(y_trainPID), 100*fails_train_Rbf/len(y_trainPID),\n",
    "              fails_Rbf, len(y_testPID), 100*fails_Rbf/len(y_testPID), \n",
    "              svmRbf.score(X_testPID, y_testPID)*100, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrida | Valor de $C$ | Error en entrenamiento mal clasificados | Puntos de prueba mal clasificados  \n",
    "----| ----| ------ | \n",
    "0   | 0.1 | 34.08% | 29.11%\n",
    "    | 0.5 | 34.08% | 29.11%\n",
    "    | 1.0 | 0.0% | 29.11%\n",
    "    | 100 | 0.0% | 29.11%\n",
    "    | 500 | 0.0% | 29.11%\n",
    "    | 1000 | 0.0% | 29.11%\n",
    "1   | 0.1 | 34.71% | 26.58%\n",
    "    | 0.5 | 34.71% | 26.58%\n",
    "    | 1.0 | 0.0% | 26.58%\n",
    "    | 100 | 0.0% | 26.58%\n",
    "    | 500 | 0.0% | 26.58%\n",
    "    | 1000 | 0.0% | 26.58%\n",
    "2   | 0.1 | 31.53% | 39.24%\n",
    "    | 0.5 | 31.53% | 39.24%\n",
    "    | 1.0 | 0.0% | 39.24%\n",
    "    | 100 | 0.0% | 39.24%\n",
    "    | 500 | 0.0% | 39.24%\n",
    "    | 1000 | 0.0% | 39.24%\n",
    "3   | 0.1 | 30.25% | 44.30%\n",
    "    | 0.5 | 30.25% | 44.30%\n",
    "    | 1.0 | 0.0% | 44.30%\n",
    "    | 100 | 0.0% | 44.30%\n",
    "    | 500 | 0.0% | 44.30%\n",
    "    | 1000 | 0.0% | 44.30%\n",
    "4   | 0.1 | 32.48% | 35.44%\n",
    "    | 0.5 | 32.48% | 35.44%\n",
    "    | 1.0 | 0.0% | 35.44%\n",
    "    | 100 | 0.0% | 35.44%\n",
    "    | 500 | 0.0% | 35.44%\n",
    "    | 1000 | 0.0% | 35.44%\n",
    "    \n",
    "En estos resultados observamos que: 1) Los resultados no son estables, con errores que oscilan entre 26.58% y 44.30% con los mismos datos y tan sólo diferentes particiones para datos de entrenamiento y de prueba. 2) No hay, en las pruebas realizadas, ningún efecto de la penalización en la clasificación de los datos de prueba, a pesar de alcanzar 0% de erorres en los datos de entrenamiento. Los resultados mejoran si normalizamos los datos:\n",
    "\n",
    "Corrida | Valor de $C$ | Error en entrenamiento mal clasificados | Puntos de prueba mal clasificados  \n",
    "----| ----| ------ | \n",
    "0   | 0.1 | 33.76% | 30.38%\n",
    "    | 0.5 | 28.03% | 29.11%\n",
    "    | 1.0 | 20.06% | 24.05%\n",
    "    | 100 | 18.15% | 24.05%\n",
    "    | 500 | 16.56% | 25.32%\n",
    "    | 1000 | 16.56% | 24.05%\n",
    "    | 5000 | 14.65% | 21.52%\n",
    "1   | 0.1 | 33.76% | 30.38%\n",
    "    | 0.5 | 27.39% | 27.85%\n",
    "    | 1.0 | 21.66% | 22.78%\n",
    "    | 100 | 19.11% | 21.52%\n",
    "    | 500 | 17.20% | 21.52%\n",
    "    | 1000 | 16.88% | 21.52%\n",
    "    | 5000 | 15.29% | 21.52%\n",
    "2   | 0.1 | 32.17% | 36.71%\n",
    "    | 0.5 | 29.62% | 37.97%\n",
    "    | 1.0 | 22.93% | 20.25%\n",
    "    | 100 | 19.75% | 21.52%\n",
    "    | 500 | 17.52% | 17.72%\n",
    "    | 1000 | 17.52% | 16.46%\n",
    "    | 5000 | 15.61% | 17.72%\n",
    "3   | 0.1 | 32.16% | 36.71%\n",
    "    | 0.5 | 32.48% | 36.71%\n",
    "    | 1.0 | 21.66% | 25.32%\n",
    "    | 100 | 19.43% | 22.78%\n",
    "    | 500 | 18.15% | 25.32%\n",
    "    | 1000 | 16.24% | 22.78%\n",
    "    | 5000 | 14.65% | 24.05%\n",
    "4   | 0.1 | 32.80% | 34.18%\n",
    "    | 0.5 | 31.53% | 34.18%\n",
    "    | 1.0 | 22.93% | 21.52%\n",
    "    | 100 | 19.43% | 21.52%\n",
    "    | 500 | 17.83% | 20.25%\n",
    "    | 1000 | 18.15% | 18.99%\n",
    "    | 5000 | 14.33% | 22.78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El otro gran parámetro de la función kernel RBF es $\\gamma$. A continuación mostramos variaciones del valor de $\\gamma$ en ejercicios de clasificación sobre los datos de diabetes originales y sobre los datos normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RBF, C=500\n",
      "Puntos mal clasificados (entrenamiento): 0 de 314 (0.0%)       \n",
      "Puntos mal clasificados (prueba): 30 de 79 (37.9746835443038%)       \n",
      "Aciertos del 62.0253164556962%\n",
      "Tiempo: 0.005285978317260742\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmRbf = SVC(kernel='rbf', C=500, gamma=0.1)\n",
    "start_time = time.time()\n",
    "svmRbf.fit(X_trainPID, y_trainPID)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "preds_train_Rbf = svmRbf.predict(X_trainPID)\n",
    "fails_train_Rbf = np.sum(y_trainPID != preds_train_Rbf)\n",
    "\n",
    "preds_Rbf = svmRbf.predict(X_testPID)\n",
    "fails_Rbf = np.sum(y_testPID != preds_Rbf)\n",
    "\n",
    "print(\"SVM RBF, C=500\\nPuntos mal clasificados (entrenamiento): {} de {} ({}%)\\\n",
    "       \\nPuntos mal clasificados (prueba): {} de {} ({}%)\\\n",
    "       \\nAciertos del {}%\\nTiempo: {}\\n\"\n",
    "      .format(fails_train_Rbf, len(y_trainPID), 100*fails_train_Rbf/len(y_trainPID),\n",
    "              fails_Rbf, len(y_testPID), 100*fails_Rbf/len(y_testPID), \n",
    "              svmRbf.score(X_testPID, y_testPID)*100, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrida | Valor de $\\gamma$ | Error en entrenamiento mal clasificados | Puntos de prueba mal clasificados  \n",
    "----| ----| ------ | \n",
    "0   | 0.1 | 24.52% | 16.46%\n",
    "    | 0.3 | 22.29% | 15.19%\n",
    "    | 0.5 | 22.29% | 16.46%\n",
    "    | 0.8 | 21.97% | 16.46%\n",
    "    | 1.0 | 21.02% | 15.19%\n",
    "1   | 0.1 | 23.25% | 20.25%\n",
    "    | 0.3 | 21.66% | 15.19%\n",
    "    | 0.5 | 21.34% | 15.19%\n",
    "    | 0.8 | 20.06% | 13.92%\n",
    "    | 1.0 | 19.43% | 13.92%\n",
    "2   | 0.1 | 25.16% | 24.05%\n",
    "    | 0.3 | 20.70% | 20.25%\n",
    "    | 0.5 | 20.70% | 20.25%\n",
    "    | 0.8 | 20.38% | 20.25%\n",
    "    | 1.0 | 19.75% | 20.25%\n",
    "3   | 0.1 | 19.75% | 32.91%\n",
    "    | 0.3 | 18.47% | 30.38%\n",
    "    | 0.5 | 18.79% | 30.38%\n",
    "    | 0.8 | 18.15% | 32.91%\n",
    "    | 1.0 | 18.47% | 31.65%\n",
    "4   | 0.1 | 23.57% | 21.52%\n",
    "    | 0.3 | 21.66% | 20.25%\n",
    "    | 0.5 | 20.70% | 22.78%\n",
    "    | 0.8 | 19.75% | 21.52%\n",
    "    | 1.0 | 19.43% | 22.78%\n",
    "    \n",
    "Los resultados no son estables, sin embargo, se aprecia cierta mejoría en algunos casos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una gran variedad de [funciones kernel](http://crsouza.com/2010/03/17/kernel-functions-for-machine-learning-applications/). Otras  dos funciones soportadas por la biblioteca *sklearn* de Python, son el kernel **[sigmoide](https://es.wikipedia.org/wiki/Función_sigmoide)** y el kernel **[polinomial](https://en.wikipedia.org/wiki/Polynomial_kernel)**. Este último, sin embargo, es poco escalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Películas:\n",
      "SVM Sigmoide, C=1.0\n",
      "Puntos mal clasificados (entrenamiento): 9991 de 20000 (49.955%)       \n",
      "Puntos mal clasificados (prueba): 2509 de 5000 (50.18%)       \n",
      "Aciertos del 49.82%\n",
      "Tiempo: 189.41826796531677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmSgm = SVC(kernel='sigmoid')\n",
    "start_time = time.time()\n",
    "svmSgm.fit(X_train, y_train)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "preds_train_Sgm = svmSgm.predict(X_train)\n",
    "fails_train_Sgm = np.sum(y_train != preds_train_Sgm)\n",
    "\n",
    "preds_Sgm = svmSgm.predict(X_test)\n",
    "fails_Sgm = np.sum(y_test != preds_Sgm)\n",
    "\n",
    "print(\"Películas:\\nSVM Sigmoide, C=1.0\\nPuntos mal clasificados (entrenamiento): {} de {} ({}%)\\\n",
    "       \\nPuntos mal clasificados (prueba): {} de {} ({}%)\\\n",
    "       \\nAciertos del {}%\\nTiempo: {}\\n\"\n",
    "      .format(fails_train_Sgm, len(y_train), 100*fails_train_Sgm/len(y_train),\n",
    "              fails_Sgm, len(y_test), 100*fails_Sgm/len(y_test), \n",
    "              svmSgm.score(X_test, y_test)*100, elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes\n",
      "SVM Sigmoide, C=1.0\n",
      "Puntos mal clasificados (entrenamiento): 100 de 314 (31.84713375796178%)       \n",
      "Puntos mal clasificados (prueba): 30 de 79 (37.9746835443038%)       \n",
      "Aciertos del 62.0253164556962%\n",
      "Tiempo: 0.0015859603881835938\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmSgm = SVC(kernel='sigmoid')\n",
    "start_time = time.time()\n",
    "svmSgm.fit(X_trainPID, y_trainPID)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "preds_train_Sgm = svmSgm.predict(X_trainPID)\n",
    "fails_train_Sgm = np.sum(y_trainPID != preds_train_Sgm)\n",
    "\n",
    "preds_Sgm = svmSgm.predict(X_testPID)\n",
    "fails_Sgm = np.sum(y_testPID != preds_Sgm)\n",
    "\n",
    "print(\"Diabetes\\nSVM Sigmoide, C=1.0\\nPuntos mal clasificados (entrenamiento): {} de {} ({}%)\\\n",
    "       \\nPuntos mal clasificados (prueba): {} de {} ({}%)\\\n",
    "       \\nAciertos del {}%\\nTiempo: {}\\n\"\n",
    "      .format(fails_train_Sgm, len(y_trainPID), 100*fails_train_Sgm/len(y_trainPID),\n",
    "              fails_Sgm, len(y_testPID), 100*fails_Sgm/len(y_testPID), \n",
    "              svmSgm.score(X_testPID, y_testPID)*100, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 3px;\">\n",
    "\n",
    "### Tarea 11\n",
    "\n",
    "* Haga una revisión de ventajas e inconvenientes de las máquinas de vectores de soporte.\n",
    "* Utilice máquinas de vectores de soporte en su proyecto.\n",
    "\n",
    "**Fecha de entrega**: Martes 18 de abril."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
