{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de patrones: Clasificación\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "![ ](images/blank.png)\n",
    "![agents](images/binary_data_under_a_magnifying.jpg)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/pattern-recognition/blob/master/Clasificación%20V.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de clasificación: Reconocimiento sintáctico\n",
    "\n",
    "\n",
    "### Reconocimiento estructural de patrones\n",
    "\n",
    "El **reconocimiento de patrones estructural** hace énfasis en la integración de patrones simples para conformar patrones complejos. En este enfoque, un patrón es descrito mediante una estructura jerárquica de componentes, como en el caso de la estructura sintáctica de los lenguajes formales:\n",
    "\n",
    "![](images/structural.jpg)<br>\n",
    "\n",
    "Existen dos formas principales de reconocimiento estructural de patrones: la **estructura de pareo** (*matching structure*) y el **análisis sintáctico**. El método de apareamiento consiste básicamente en *aparear* los elementos a clasificar con *moldes* conocidos. En muchos casos, este método puede expresarse de manera sintáctica. \n",
    "\n",
    "![](images/matching.jpg)\n",
    "\n",
    "El enfoque sintáctico, por su parte, realiza el reconocimiento de patrones a partir de una determinada *sintaxis*, lo que permite utilizar las herramientas de la teoría formal de lenguajes. \n",
    "\n",
    "### Gramáticas formales\n",
    "\n",
    "Una gramática (formal) se define como la tupla\n",
    "\n",
    "$$G = (N,\\Sigma,P,S)$$\n",
    "\n",
    "donde\n",
    "\n",
    "* $N$ es un conjunto finito de símbolos  no-terminales (variables a substituir)\n",
    "* $\\Sigma$ es un conjunto finito de símbolos terminales llamado el *alfabeto* o *vocabulario*\n",
    "* $S\\in N$ es el *símbolo inicial*, es decir el símbolo no terminal desde donde se inicia la construcción de una *'frase'*\n",
    "* $P$ es un conjunto finito de *reglas de producción*, es decir, reglas que definen cómo pueden irse reemplazando los símbolos no-terminales, desde el símbolo inicial, hasta tener una frase terminada.\n",
    "\n",
    "La forma de las reglas de producción determinan el tipo de gramática y el correspondiente autómata. \n",
    "\n",
    "Cada gramática está asociado a un tipo de *autómata* que sería, en realidad, el responsable de reconocer los patrones generables por la gramática correspondiente.\n",
    "\n",
    "Un lenguaje, es un conjunto de secuencias o cadenas sobre $\\Sigma$: $L(G) \\subseteq \\Sigma^*$. Cuando se aplica la teoría de gramáticas formales al lenguaje natural, el vocabulario es usualmente un conjunto de letras, signos, palabras, morfemas o sonidos.\n",
    "\n",
    "Los tipos principales de autómata están definidos mediante la jerarquía de Chomsky:\n",
    "\n",
    "Gramática | Lenguaje | Autómata\n",
    "-| \n",
    "Tipo 0 | Recursivamente enumerable |\tMáquina de Turing\n",
    "Tipo 1 | Dependiente del contexto | Autómata linealmente acotado\n",
    "Tipo 2 | Independiente del contexto | Autómata de pila\n",
    "Tipo 3 | Regular | Autómata finito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gramáticas no lingüísticas\n",
    "\n",
    "Aunque la formulación de gramáticas proviene del contexto lingüístico, es posible desarrollar gramáticas para representar patrones en otros contextos. Consideremos por ejemplo la siguiente gramática regular:\n",
    "\n",
    "![](images/syntactic_1.png)<br>\n",
    "\n",
    "En esta gramática, los elementos del alfabeto son segmentos de rectángulos. A partir de las reglas en $P$ podemos construir rectángulos como los siguientes:<br>\n",
    "\n",
    "![](images/syntactic_2.png)\n",
    "\n",
    "Dada esta gramática podemos construir un autómata finito capaz de reconocer rectángulos en una imagen. \n",
    "\n",
    "Un problema que ha llamado intensamente la atención de la industria es el reconocimiento automático de placas vehiculares. Este es un problema relativamente simple de resolver en ambientes controlados, sin embargo, es un problema complicado cauando deben reconocerse placas de diferentes tipos en un contexto abierto, posiblemente con visibiidad limitada:\n",
    "\n",
    "![](images/plates.jpg)\n",
    "\n",
    "Un paso importante en la resolución de este problema es identificar la *estructura* en los componentes de una placa: Una placa de auto es un rectángulo (el símbolo inicial $S$):\n",
    "\n",
    "![](images/plates_1.jpg)\n",
    "\n",
    "Pero no cualquier rectángulo \"*genera*\" una placa:\n",
    "\n",
    "![](images/plates_2.jpg)\n",
    "\n",
    "\n",
    "\n",
    "Una etapa posterior incluiría evaluar las posibles producciones a partir del rectángulo, capaces de conducir a la generación de una placa válida:\n",
    "\n",
    "![](images/plates_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de lenguaje natural\n",
    "\n",
    "#### Gramáticas libres de contexto\n",
    "\n",
    "La ubicación de los lenguajes naturales en la jerarquía de Chomsky (o qué tanto pueden ser representados en ella) es un tema de discusión abierto. El uso de los diferentes tipos de gramáticas para análisis de lenguajes naturales ha sido limitado, siendo las gramáticas más utilizadas las gramáticas libres de contexto.\n",
    "\n",
    "Considérese la siguiente gramática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.parse.generate import generate\n",
    "from nltk import CFG\n",
    "from IPython.display import Image, display  \n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | NP PP | N\n",
    "    VP -> V NP | VP PP | V\n",
    "    PP -> P NP | P\n",
    "    Det -> 'el' | 'los' | 'la'\n",
    "    N -> 'hombre' | 'parque' | 'perro' | 'amigos' | 'cafe' | 'leche'\n",
    "    V -> 'duerme' | 'mira' | 'toma' | 'camina' | 'toman'\n",
    "    P -> 'en' | 'con' |'solo'\n",
    "    \"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta gramática permite generar frases como $\\textrm{\"el perro duerme\"}$, cuyo árbol de generación por $G$\n",
    "es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = \"el perro duerme\"\n",
    "print(\"Árbol de generación de la cadena \\\"{}\\\"\".format(X))\n",
    "for tree in parser.parse(X.split()):\n",
    "    display(tree) # tree.draw() arroja una ventana emergente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o $\\textrm{\"los amigos toman cafe\"}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = \"los amigos toman cafe\"\n",
    "print(\"Generación de la cadena \\\"{}\\\"\".format(X))\n",
    "for tree in parser.parse(X.split()):\n",
    "    display(tree)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o $\\textrm{\"el perro duerme en el parque\"}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = \"el perro duerme en el parque\"\n",
    "print(\"Generación de la cadena \\\"{}\\\"\".format(X))\n",
    "for tree in parser.parse(X.split()):\n",
    "    display(tree)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y muchas otras frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_to_print = 30\n",
    "frases = generate(grammar)\n",
    "\n",
    "for sentence in generate(grammar, n=sent_to_print):\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas de esta frases, aunque son sintácticamente correctas, no tienen un significado \"correcto\". \n",
    "\n",
    "Las gramáticas libres de contexto (y los correspondientes autómatas finitos / de pila) ofrecen un mecanismo poderoso para la generación (y reconocimiento) de patrones, particularmente útiles en el reconocimiento de patrones en lenguajes, por ejemplo para la implementación de compiladores (específicamente en la etapa de *parsing* o análisis sintáctico).\n",
    "\n",
    "#### Gramáticas sensibles al contexto\n",
    "\n",
    "Las gramáticas sensibles al contexto ofrecen una mayor capacidad de discriminación al establecer condiciones de contexto para la aplicación de reglas. Las reglas, en este caso, contienen cadenas en ambos lados, del tipo $\\alpha\\textrm{A}\\beta \\to \\alpha \\gamma \\beta$ donde $\\textrm{A} \\in N$, $\\alpha, \\beta \\in (N \\cup \\Sigma)^*$ y $\\gamma \\in (N \\cup \\Sigma)^+$. En la siguiente versión modificada de nuestra gramática, hemos reemplazado la regla $\\textrm{NP} \\to \\textrm{N}$ por las reglas $\\textrm{'la' NP} \\to \\textrm{NF}$ y $\\textrm{'la' NP} \\to \\textrm{NF}$ y hemos distinguido entre nombres femenino ($\\textrm{NF}$) y masculino ($\\textrm{NM}$). \n",
    "\n",
    "    P = {\n",
    "        S -> NP VP\n",
    "        NP -> Det N\n",
    "        NP -> NP PP\n",
    "        'la' NP -> NF\n",
    "        'el' NP -> NM\n",
    "        VP -> V NP\n",
    "        VP -> VP PP\n",
    "        VP -> V\n",
    "        PP -> P NP\n",
    "        PP -> P\n",
    "        Det -> 'el'\n",
    "        Det -> 'los'\n",
    "        Det -> 'la'S -> NP VP\n",
    "        NM -> 'hombre'\n",
    "        NM -> 'parque'\n",
    "        NM -> 'perro'\n",
    "        NM -> 'amigos'\n",
    "        NM -> 'cafe'\n",
    "        NF -> 'leche'\n",
    "        V -> 'duerme'\n",
    "        V -> 'mira'\n",
    "        V -> 'toma'\n",
    "        V -> 'camina'\n",
    "        V -> 'toman'\n",
    "        P -> 'en'\n",
    "        P -> 'con'\n",
    "        P -> 'solo'\n",
    "    }\n",
    "    \n",
    "Estos cambios evitarían la generación de frases como \"el hombre duerme <u>la hombre</u>\", \"el hombre duerme <u>la parque</u>\", \"el hombre duerme <u>la perro</u>\", \"el hombre duerme <u>la cafe</u>\" y \"el hombre duerme el hombre en <u>el leche</u>\". Sin embargo, tratar de reflejar el contexto en frases generadas en un lenguaje natural mediante reglas rebasa la capacidad de las gramáticas convencionales. Considérese el siguiente ejemplo clásico en un diálogo de *Groucho Marx* (*Animal Crackers*, 1930):\n",
    "\n",
    "> *One morning I shot an elephant in my pajamas. <br>\n",
    "> How he got in my pajamas, I don't know.* ![](images/groucho.jpg)\n",
    "\n",
    "Aunque la situación es utilizada como broma, particularmente al ser forzada por Groucho Marx, la estructura es sintácticamente correcta, lo cual es claro si modificamos ligeramente la cita, de la siguiente manera:\n",
    "\n",
    "> I shot an elephant in my yard.\n",
    "\n",
    "Una gramática (de juguete) capaz de generar estas frases sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP \n",
    "    NP -> Det N | Det N PP | 'I'\n",
    "    VP -> V NP | VP PP \n",
    "    Det -> 'an' | 'my'\n",
    "    N -> 'elephant' | 'pajamas' | 'yard'\n",
    "    V -> 'shot'\n",
    "    P -> 'con' | 'in'\n",
    "    \"\"\")\n",
    "\n",
    "print(groucho_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta gramática genera dos árboles en cada caso. Así para la frase \"*I shot an elephant in my yard*\", los árboles de generación serían:\n",
    "\n",
    "![](images/groucho_elephant.png)\n",
    "\n",
    "El árbol de la izquierda tiene al mismo nivel el verbo ('*shot*') y la preposición ('*in*'); describe la realización de la acción en un sitio \"SHOT ... IN...\": \"*Le disparé a un elefante cuando yo estaba en mi patio*\". El árbol de la derecha pone a la misma altura el objeto nominal ('*an elephant*') y a la frase preposicional ('*in my yard*'): \"*Le disparé a un elefante que estaba en mi patio*\". Ambas frases son sintácticamente y semánticamente correctas.\n",
    "\n",
    "Los árboles de generación para la frase original, \"*I shot an elephant in my pajamas*\", son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string = \"I shot an elephant in my pajamas\"\n",
    "\n",
    "groucho_parser = nltk.ChartParser(groucho_grammar)\n",
    "for tree in groucho_parser.parse(string.split()):\n",
    "    display(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la gramática de prueba, la frase $\\textrm{\"el hombre con el perro camina en el parque con amigos\"}$ también tiene asociados dos árboles de generación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = \"el hombre con el perro camina en el parque con amigos\"\n",
    "print(\"Generación de la cadena \\\"{}\\\"\".format(X))\n",
    "for tree in parser.parse(X.split()):\n",
    "    display(tree)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque las gramáticas sensibles al contexto ofrecen una manera elegante y natural de extender las gramáticas regulares para tomar en consideración el contexto en una producción, en la práctica son de poca utilidad para lidiar con problemas de lenguaje natural. Entre las limitaciones de las gramáticas sensibles al contexto destaca su complejidad computacional, particularmente su caracter [PSPACE-complete](https://en.wikipedia.org/wiki/PSPACE-complete) (también, es notable la rapidez con que una gramática sensible al contexto se vuelve difícil de describir y la tasa en que aumenta la cantidad de reglas conforme se agregan características del contexto). Por ello, se han propuesto otras formas de gramáticas que, sin poseer el poder de una gramática sensible al contexto, son capaces de describir elementos del contexto en una frase. Destacan entre tales gramáticas las gramáticas probabilísticas.\n",
    "\n",
    "\n",
    "### Naturaleza probabilística del lenguaje natural\n",
    "\n",
    "Una característica de los lenguajes naturales es que las frases que los componen no tienen una distribución uniforme. Por el contrario, existen construcciones que son más comunes que otras. De esta manera, aunque las frases $f_1 = \\textrm{\"el perro camina\"}$ y $f_2 = \\textrm{\"el perro vuela\"}$ son ambas correctas sintácticamente, es más probable encontrar la frase $f_1$ que la frase $f_2$ en un texto arbitrario. \n",
    "\n",
    "Esta característica estadística es modelada mediante las llamadas \"*gramáticas libres de contexto probabilísticas*\". Una **Gramática libre de contexto probabilística** (o estocástica) es una **Gramática libre de contexto** cuyas reglas de producción tienen asignadas probabilidades de aplicación, de manera que la suma de probabilidades para todas las reglas que expanden el mismo símbolo no terminal es uno. Considérese la siguiente variante de nuestra gramática ejemplo\n",
    "\n",
    "![](images/pcfg.png)\n",
    "\n",
    "Estas reglas generan un lenguaje centrado en el tema de $\\textrm{\"hombre o perro\"}$ en el $\\textrm{\"parque\"}$.\n",
    "\n",
    "Aunque las gramáticas libres de contexto probabilísticas resultan más adecuadas para generar y reconocer lenguajes naturales que las gramáticas no probabilísticas, aún carecen del poder para capturar las dependencias típicas de los lenguajes naturales. Sin embargo, estas gramáticas ofrecen un avance y nuevas ideas de cómo abordar el problema de identificación del contexto en una frase.\n",
    "\n",
    "### N-gramas y el enfoque probabilístico\n",
    "\n",
    "En la película \"*Take The Money And Run*\", Virgil Starkwell (Woody Allen) intenta asaltar un banco y entrega al cajero una nota con el mensaje \"*Please put fifty thousand dollars into this bag and act natural as I am pointing a gun at you*\" que es leída por los empleados del banco como \"*Please put fifty thousand dollars into this bag and ABT natural as I am pointing a GUB at you*\". \n",
    "\n",
    "[![](images/i_have_a_gub.jpg)](https://www.youtube.com/watch?v=pEm0zi8QrpA)\n",
    "\n",
    "Sin embargo, es obvio que la frase \"*I am pointing a GUN at you*\" es más probable que la frase \"*I am pointing a GUB at you*\", por lo que en la vida real no nos cuesta trabajo reconocer la frase correcta. Para modelar esta capacidad de predecir la ocurrencia de una palabra en una frase se utilizan **Modelos de lenguajes** que asignan probabilidades a las secuencias de palabras que pueden conformar un texto. \n",
    "\n",
    "El modelo más simple es el **Modelo de N-Gramas**\". Este modelo asume que la probabiliad de ocurrencia de una palabra está determinada por las palabras recientes; lo que se conoce como la **suposición de Markov**. De manera que para el cálculo de estas probabilidades basta contabilizar la ocurrencia de secuencias de palabras de longitud definida. Un **$N$-Grama** es una secuencia de $N$ palabras. Así, por ejemplo, un 2-grama (o bigrama) es una secuencia de 2 palabras, como \"*el hombre*\", \"*hombre camina*\", \"*camina en*\", \"*en el*\", \"*el parque*\". Un 3-grama (trigrama) es una secuencia de tres palabras, como \"*el hombre camina*\", \"*hombre camina en*\", \"*camina en el*\", \"*en el parque*\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"los amigos toman cafe. el perro duerme en el parque. el hombre con el perro \\\n",
    "camina en el parque con amigos.\"\n",
    "\n",
    "token = nltk.word_tokenize(text)\n",
    "bigrams = ngrams(token,2)\n",
    "trigrams = ngrams(token,3)\n",
    "fourgrams = ngrams(token,4)\n",
    "fivegrams = ngrams(token,5)\n",
    "\n",
    "bigrams_list = list(bigrams)\n",
    "counter_bigrams = Counter(bigrams_list)\n",
    "\n",
    "print (bigrams_list)\n",
    "print (counter_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from nltk.corpus import stopwords \n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "os.chdir('Data sets/Movies Reviews')\n",
    "movies_reviews = pd.read_csv(\"labeledTrainData.tsv\", sep='\\t')\n",
    "\n",
    "# Limpiar los documentos. Conservar sólo plabras (alfabéticas) y pasar a minúsculas\n",
    "movies_reviews.review = list(map(lambda row: re.sub(\"[^a-zA-Z]\", \" \", \n",
    "                                BeautifulSoup(row, \"lxml\").get_text().lower()), \n",
    "                                 movies_reviews.review))\n",
    "\n",
    "# Agregar una columna con la conversión de mensajes a listas de palabras\n",
    "# Se eliminan las palabras vacías\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "movies_reviews[\"words\"] = list(map(lambda row: [w for w in row.split() if not w in stops], \n",
    "                                   movies_reviews.review))\n",
    "\n",
    "movies_reviews[\"bigrams\"] = list(map(lambda row: list(ngrams(word_tokenize(row),2)), \n",
    "                                   movies_reviews.review))\n",
    "\n",
    "movies_reviews[\"bigrams_sw\"] = list(map(lambda row: list(ngrams(row,2)), \n",
    "                                   movies_reviews.words))\n",
    "\n",
    "\n",
    "display(movies_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generar un arreglo con los valores de clasificación\n",
    "Sentiments = np.array([int(x) for x in movies_reviews.sentiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construcción de la Bolsa de palabras. Se seleccionan las 4000 palabras más frecuentes\n",
    "all_words = nltk.FreqDist(w.lower() for wl in movies_reviews.words for w in wl)\n",
    "print(\"50 palabras más populares:\\n\", all_words.most_common(50))\n",
    "\n",
    "# Construcción de la Bolsa de palabras. Se seleccionan las 4000 palabras más frecuentes\n",
    "all_bigrams = nltk.FreqDist(w for wl in movies_reviews.bigrams for w in wl)\n",
    "print(\"50 palabras más populares:\\n\", all_bigrams.most_common(50))\n",
    "\n",
    "# Construcción de la Bolsa de palabras. Se seleccionan las 4000 palabras más frecuentes\n",
    "all_bigrams_sw = nltk.FreqDist(w for wl in movies_reviews.bigrams_sw for w in wl)\n",
    "print(\"50 palabras más populares:\\n\", all_bigrams_sw.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features = [ w for (w,f) in all_words.most_common(4000)]\n",
    "\n",
    "bigrams_features = [ w for (w,f) in all_bigrams.most_common(4000)]\n",
    "\n",
    "bigrams_sw_features = [ w for (w,f) in all_bigrams_sw.most_common(4000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Regresa el vector de características de un documento\n",
    "def document_features(document, global_features): \n",
    "    document_words = set(document) \n",
    "    features = []\n",
    "    for word in global_features:\n",
    "        if (word in document_words) :\n",
    "            features.append(1)\n",
    "        else :\n",
    "            features.append(0)\n",
    "    return features\n",
    "\n",
    "\n",
    "# Vectores de características de la colección de documentos\n",
    "featuresets_words = [\n",
    "    document_features(d, word_features) for d in movies_reviews[\"words\"]]\n",
    "featuresets_bigrams = [\n",
    "    document_features(d, bigrams_features) for d in movies_reviews[\"bigrams\"]]\n",
    "featuresets_bigrams_sw = [\n",
    "    document_features(d, bigrams_sw_features) for d in movies_reviews[\"bigrams_sw\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(sum(x > 0 for x in featuresets_words[i]), \n",
    "          sum(x > 0 for x in featuresets_bigrams[i]),\n",
    "          sum(x > 0 for x in featuresets_bigrams_sw[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "# Dividir datos en dos conjuntos: entrenamiento y prueba\n",
    "words_train, words_test, wy_train, wy_test = train_test_split(\n",
    "    featuresets_words, Sentiments, test_size=0.2)\n",
    "\n",
    "# Entrenamiento de un clasificador Bernouilli Bayes ingenuo\n",
    "#clfB = BernoulliNB(alpha=1.0, class_prior=None, fit_prior=False)\n",
    "clfBw = BernoulliNB()\n",
    "clfBw.fit(words_train, wy_train)\n",
    "\n",
    "# Pruebas del clasificador\n",
    "predictions_train_words = clfBw.predict(words_train)\n",
    "fails_train_words = np.sum(wy_train != predictions_train_words)\n",
    "print(\"Puntos mal clasificados en el conjunto de entrenamiento: {} de {} ({}%)\\n\"\n",
    "      .format(fails_train_words, len(words_train), 100*fails_train_words/len(words_train)))\n",
    "predictions_test_words = clfBw.predict(words_test)\n",
    "fails_test_words = np.sum(wy_test != predictions_test_words)\n",
    "print(\"Puntos mal clasificados en el conjunto de prueba: {} de {} ({}%)\\n\"\n",
    "      .format(fails_test_words, len(words_test), 100*fails_test_words/len(words_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dividir datos en dos conjuntos: entrenamiento y prueba\n",
    "bigrams_train, bigrams_test, biy_train, biy_test = train_test_split(\n",
    "    featuresets_bigrams, Sentiments, test_size=0.2)\n",
    "\n",
    "# Entrenamiento de un clasificador Bernouilli Bayes ingenuo\n",
    "#clfB = BernoulliNB(alpha=1.0, class_prior=None, fit_prior=False)\n",
    "clfBbi = BernoulliNB()\n",
    "clfBbi.fit(bigrams_train, biy_train)\n",
    "\n",
    "# Pruebas del clasificador\n",
    "predictions_train_bigrams = clfBbi.predict(bigrams_train)\n",
    "fails_train_bigrams = np.sum(biy_train != predictions_train_bigrams)\n",
    "print(\"Puntos mal clasificados en el conjunto de entrenamiento: {} de {} ({}%)\\n\"\n",
    "      .format(fails_train_bigrams, len(bigrams_train), \n",
    "              100*fails_train_bigrams/len(bigrams_train)))\n",
    "predictions_test_bigrams = clfBbi.predict(bigrams_test)\n",
    "fails_test_bigrams = np.sum(biy_test != predictions_test_bigrams)\n",
    "print(\"Puntos mal clasificados en el conjunto de prueba: {} de {} ({}%)\\n\"\n",
    "      .format(fails_test_bigrams, len(bigrams_test), \n",
    "              100*fails_test_bigrams/len(bigrams_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dividir datos en dos conjuntos: entrenamiento y prueba\n",
    "bigrams_sw_train, bigrams_sw_test, biswy_train, biswy_test = train_test_split(\n",
    "    featuresets_bigrams_sw, Sentiments, test_size=0.2)\n",
    "\n",
    "# Entrenamiento de un clasificador Bernouilli Bayes ingenuo\n",
    "#clfB = BernoulliNB(alpha=1.0, class_prior=None, fit_prior=False)\n",
    "clfBbisw = BernoulliNB()\n",
    "clfBbisw.fit(bigrams_sw_train, biswy_train)\n",
    "\n",
    "# Pruebas del clasificador\n",
    "predictions_train_bigrams_sw = clfBbisw.predict(bigrams_sw_train)\n",
    "fails_train_bigrams_sw = np.sum(biswy_train != predictions_train_bigrams_sw)\n",
    "print(\"Puntos mal clasificados en el conjunto de entrenamiento: {} de {} ({}%)\\n\"\n",
    "      .format(fails_train_bigrams_sw, len(bigrams_sw_train), \n",
    "              100*fails_train_bigrams_sw/len(bigrams_sw_train)))\n",
    "predictions_test_bigrams_sw = clfBbisw.predict(bigrams_sw_test)\n",
    "fails_test_bigrams_sw = np.sum(biswy_test != predictions_test_bigrams_sw)\n",
    "print(\"Puntos mal clasificados en el conjunto de prueba: {} de {} ({}%)\\n\"\n",
    "      .format(fails_test_bigrams_sw, len(bigrams_sw_test), \n",
    "              100*fails_test_bigrams_sw/len(bigrams_sw_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies_reviews = pd.read_csv(\"labeledTrainData.tsv\", sep='\\t')\n",
    "\n",
    "# Limpiar los documentos. Conservar sólo plabras (alfabéticas) y pasar a minúsculas\n",
    "movies_reviews.review = list(map(lambda row: re.sub(\"[^a-zA-Z]\", \" \", \n",
    "                                BeautifulSoup(row, \"lxml\").get_text().lower()), \n",
    "                                 movies_reviews.review))\n",
    "\n",
    "# Agregar una columna con la conversión de mensajes a listas de palabras\n",
    "# Sin eliminar las palabras vacías\n",
    "movies_reviews[\"words\"] = list(map(lambda row: row.split(), movies_reviews.review))\n",
    "\n",
    "most_common_words = nltk.FreqDist(w for wl in movies_reviews.words for w in wl)\n",
    "print(\"30 palabras más populares:\\n\", most_common_words.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'movie' es la primera palabra de interés y tiene una frecuencia de 44031... eliminamos las primeras 15 palabras.... y volvemos a crear los bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_stop_words = [ w for (w,f) in most_common_words.most_common(15)]\n",
    "\n",
    "movies_reviews[\"words\"] = list(map(lambda row: \n",
    "                                   [w for w in row.split() if not w in my_stop_words], \n",
    "                                   movies_reviews.review))\n",
    "\n",
    "movies_reviews[\"bigrams\"] = list(map(lambda row: list(ngrams(row,2)), \n",
    "                                   movies_reviews.words))\n",
    "\n",
    "movies_reviews[\"trigrams\"] = list(map(lambda row: list(ngrams(row,3)), \n",
    "                                   movies_reviews.words))\n",
    "\n",
    "display(movies_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construcción de la Bolsa de palabras. Se seleccionan las 4000 palabras más frecuentes\n",
    "bigrams = nltk.FreqDist(w for wl in movies_reviews.bigrams for w in wl)\n",
    "bigrams_features = [ w for (w,f) in bigrams.most_common(4000)]\n",
    "featuresets_bigrams = [\n",
    "    document_features(d, bigrams_features) for d in movies_reviews[\"bigrams\"]]\n",
    "\n",
    "trigrams = nltk.FreqDist(w for wl in movies_reviews.trigrams for w in wl)\n",
    "trigrams_features = [ w for (w,f) in trigrams.most_common(4000)]\n",
    "featuresets_trigrams = [\n",
    "    document_features(d, trigrams_features) for d in movies_reviews[\"trigrams\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dividir datos en dos conjuntos: entrenamiento y prueba\n",
    "bigrams_train, bigrams_test, biy_train, biy_test = train_test_split(\n",
    "    featuresets_bigrams, Sentiments, test_size=0.2)\n",
    "\n",
    "# Entrenamiento de un clasificador Bernouilli Bayes ingenuo\n",
    "#clfB = BernoulliNB(alpha=1.0, class_prior=None, fit_prior=False)\n",
    "clfBbi = BernoulliNB()\n",
    "clfBbi.fit(bigrams_train, biy_train)\n",
    "\n",
    "# Pruebas del clasificador\n",
    "predictions_train_bigrams = clfBbi.predict(bigrams_train)\n",
    "fails_train_bigrams = np.sum(biy_train != predictions_train_bigrams)\n",
    "print(\"Puntos mal clasificados en el conjunto de entrenamiento: {} de {} ({}%)\\n\"\n",
    "      .format(fails_train_bigrams, len(bigrams_train), \n",
    "              100*fails_train_bigrams/len(bigrams_train)))\n",
    "predictions_test_bigrams = clfBbi.predict(bigrams_test)\n",
    "fails_test_bigrams = np.sum(biy_test != predictions_test_bigrams)\n",
    "print(\"Puntos mal clasificados en el conjunto de prueba: {} de {} ({}%)\\n\"\n",
    "      .format(fails_test_bigrams, len(bigrams_test), \n",
    "              100*fails_test_bigrams/len(bigrams_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "movies_reviews = pd.read_csv(\"labeledTrainData.tsv\", sep='\\t')\n",
    "\n",
    "# Limpiar los documentos. Conservar sólo plabras (alfabéticas) y pasar a minúsculas\n",
    "movies_reviews.review = list(map(lambda row: re.sub(\"[^a-zA-Z]\", \" \", \n",
    "                                BeautifulSoup(row, \"lxml\").get_text().lower()), \n",
    "                                 movies_reviews.review))\n",
    "\n",
    "# Agregar una columna con la conversión de mensajes a listas de palabras\n",
    "# Sin eliminar las palabras vacías\n",
    "movies_reviews[\"words\"] = list(map(lambda row: row.split(), movies_reviews.review))\n",
    "\n",
    "# Agregar una columna con la conversión de mensajes a listas de palabras\n",
    "# Se eliminan las palabras vacías\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "movies_reviews[\"words\"] = list(map(lambda row: [w for w in row.split() if not w in stops], \n",
    "                                   movies_reviews.review))\n",
    "\n",
    "movies_reviews[\"bigrams\"] = list(map(lambda row: list(ngrams(row,2)), \n",
    "                                   movies_reviews.words))\n",
    "\n",
    "movies_reviews[\"trigrams\"] = list(map(lambda row: list(ngrams(row,3)), \n",
    "                                   movies_reviews.words))\n",
    "\n",
    "\n",
    "words_frq = nltk.FreqDist(w.lower() for wl in movies_reviews.words for w in wl\n",
    "                         ).most_common(4000)\n",
    "bigrams_frq = nltk.FreqDist(w for wl in movies_reviews.bigrams for w in wl\n",
    "                           ).most_common(4000)\n",
    "trigrams_frq = nltk.FreqDist(w for wl in movies_reviews.trigrams for w in wl\n",
    "                            ).most_common(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def document_features_ngrams(document, global_features): \n",
    "    document_ngrams = nltk.FreqDist(w for wl in movies_reviews.bigrams for w in wl) \n",
    "    features = [0] * len(global_features)\n",
    "    for index, (elem, f) in enumerate(global_features):\n",
    "        tf = np.log(document_ngrams.freq(elem))\n",
    "        idf = np.log(1 / f)\n",
    "        features[index] = tf * idf\n",
    "    return features\n",
    "\n",
    "featuresets_bigrams = [\n",
    "    document_features_ngrams(d, bigrams_frq) for d in movies_reviews[\"bigrams\"]]\n",
    "\n",
    "print(featuresets_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectores de características de la colección de documentos\n",
    "featuresets_words = [\n",
    "    document_features(d, words_frq.most_common(4000)) for d in movies_reviews[\"words\"]]\n",
    "featuresets_bigrams = [\n",
    "    document_features(d, bigrams_frq) for d in movies_reviews[\"bigrams\"]]\n",
    "featuresets_trigrams = [\n",
    "    document_features(d, trigrams_frq) for d in movies_reviews[\"trigrams\"]]\n",
    "\n",
    "\n",
    "\n",
    "Sentiments = np.array([int(x) for x in movies_reviews.sentiment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<hr style=\"border-width: 3px;\">\n",
    "\n",
    "### Tarea 12\n",
    "\n",
    "* Haga una revisión de ventajas e inconvenientes de las redes neuronales feed-forward.\n",
    "* Utilice la técnica de redes neuronales en su proyecto.\n",
    "\n",
    "**Fecha de entrega**: Martes 16 de noviembre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
