{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de patrones: Clasificación\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "![ ](images/blank.png)\n",
    "![agents](images/binary_data_under_a_magnifying.jpg)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/pattern-recognition/blob/master/Clasificación%20V.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de clasificación: Redes Neuronales\n",
    "\n",
    "\n",
    "### Las metáforas en aprendizaje automático\n",
    "<font style=\"color:#0011d2\">**[Metáfora](http://dle.rae.es/?id=P4sce2c)**</font>\n",
    "<p style=\"background-color:#e8e8e8; color:#008100\">Del lat. metaphŏra, y este del gr. μεταφορά metaphorá.\n",
    "\n",
    "1. <font style=\"color:#006ec3\">f. *Ret.*</font> Traslación del sentido recto de una voz a otro figurado, en virtud de una comparación tácita, como en las perlas del rocío, la primavera de la vida o refrenar las pasiones. ([RAE](http://dle.rae.es/?id=P4sce2c))<br>\n",
    "\n",
    "Las metáforas son uno de los dispositivos más utilizados como figuras literarias. Una metáfora se refiere a un significado o a la identidad atribuida a un sujeto por medio de otro. ([Figuras Literarias](http://figurasliterarias.org/content/metáfora))<br>\n",
    "\n",
    "![](images/metaphors.jpg)<br>\n",
    "\n",
    "El uso de una metáfora no se entiende como una comparación fiel, en un sentido directo. <br>\n",
    "\n",
    "![](images/metaphors2.jpg)<br>\n",
    "\n",
    "En el caso de **aprendizaje automático**, se utilizan metáforas con diversos sistemas naturales/sociales para desarrollar sistemas artificiales cuyo funcionamiento recuerda \"*de alguna manera*\" al funcionamiento de sus contrapartes metafóricas.<br>\n",
    "\n",
    "El uso de metáforas para el desarrollo de sistemas artificiales se originó en el trabajo de Arturo Rosenblueth y Norbert Wiener. Su colaboración dió origen a la cibernética y, de ahí, a la inteligencia artificial. Algunas referencias importantes que documentan esta colaboración son:\n",
    "\n",
    "* [Behavior, Purpose and Teleology](http://cleamc11.vub.ac.be/Books/Wiener-teleology.pdf), 1943; por Arturo Rosenblueth, Norbert Wiener y Julian Bigelow. El documento que inició todo.\n",
    "* [Arturo Rosenblueth y Norbert Wiener: dos científicos en la historiografía de la educación contemporánea](http://cleamc11.vub.ac.be/Books/Wiener-teleology.pdf)\n",
    "* [Coloquio: Norbert Wiener y Arturo Rosenblueth: un encuentro interdisciplinario](http://computo.ceiich.unam.mx/webceiich/deptoDifusion/32PreMed/12-05-B2.pdf)\n",
    "* [Analogías entre hombre y máquina. El Grupo Cibernética y algunas de sus ideas fundacionales](http://www.con-temporanea.inah.gob.mx/node/42)\n",
    "\n",
    "![](images/rosenblueth-wiener.jpg)<br>\n",
    "\n",
    "El primer resultado importante del uso de metáforas biológicas para el desarrollo de sistemas automatizados fue la Neurona de McCulloch-Pitts que daría origen a las redes neuronales. \n",
    "\n",
    "### Neurona biológica\n",
    "\n",
    "La neurona es la célula base del sistema nervioso. Su característica más importante es la excitabilidad eléctrica de su membrana plasmática. Esta peculiaridad le ha permitido desarrollar una especialización en la recepción de estímulos eléctricos y la transmisión de esta información en forma de impulsos eléctricos. Los estímulos pueden provenir de otras neuronas o de otros tipos de células, por ejemplo células sensoriales. El impulso nervioso es envido a otras neuronas o hacia otros tipos de células, por ejemplo, hacia las fibras musculares. <br><br>\n",
    "\n",
    "![](images/Complete_neuron_cell_diagram_es.png)<br>\n",
    "\n",
    "Podemos simplificar la estructura de la neurona de la siguiente manera:<br>\n",
    "\n",
    "![](images/neuron.png)\n",
    "\n",
    "La característica morfológic más distintiva de un neurona como célula, es el conjunto de \"extensiones\" de su membrana: las dendritas y los axones. Las dendritas tienen la función de recibir los estímulos provenientes de otras neuronas en forma de neurotransmisores químicos y convertirlos en potenciales eléctricos en la membrana. Cuando el potencial eléctrico en la membrana es suficientemente alto, es decir, cuando se alcanza un cierto umbral de excitación en la membrana, se modifica la permeabilidad de ésta provocando un flujo de iones hacia adentro y afuera de la célula. Este movimiento de iones a través de la membrana genera una corriente eléctrica que a su vez provoca un tren de cambios locales de permeabilidad a lo largo de todo el axón. Este tren de *potenciales de acción* constituye el impulso eléctrico que, de esta manera, \"viaja\" desde el cuerpo celular hasta las *sinapsis* en las dendritas terminales en el axón. Al final del recorrido, las señales eléctricas provocan la liberación de substancias químicas (los neurotransmisores) que tendrán la función de excitar las dendritas de las neuronas vecinas, logrando de esta manera que la señal viaje a lo largo de un determinado camino por el sistema nervioso.\n",
    "\n",
    "<Font style=\"color:red\">**NOTA:**</Font> Una descripción más detallada en el curso de **Redes Neuronales** ;-) \n",
    "\n",
    "### Neurona de McCulloch-Pitts\n",
    "\n",
    "El primer modelo de neurona artificial fue propuesto por [Warren S. McCulloch](https://en.wikipedia.org/wiki/Warren_Sturgis_McCulloch) (neurocientífico) y [Walter Pitts](https://en.wikipedia.org/wiki/Walter_Pitts) (matemático) en 1943 en el artículo \"[A logical calculus of the ideas immanent in nervous activity](http://www.minicomplexity.org/pubs/1943-mcculloch-pitts-bmb.pdf)\". Esta neurona es denominada **neurona de McCulloch-Pitts**, *neuronas MCP*, o *unidad/compuerta lógica de umbral*.\n",
    "\n",
    "![](images/mcp-neuron.png)\n",
    "\n",
    "\n",
    "Este modelo implementa la metáfora neuronal de la siguiente manera:\n",
    "\n",
    "1. Las señales provenientes del ambiente o de otras neuronas se modelan como el conjunto de entradas $\\{x_1, x_2, \\ldots, x_n\\}$. \n",
    "2. Cada una de las señales de entrada ejerce un estímulo en la neurona que depende de la actividad sináptica, la que a su vez codifica qué tanta importancia da la neurona a una determinada señal. En el modelo, esta importancia se representa mediante el conjunto de *pesos sinápticos* $\\{w_1, w_2, \\ldots, w_n\\}$.\n",
    "3. Los estímulos provenientes de cada entrada (la señal ponderada por el peso), son integrados en el cuerpo celular. Dado que, en este caso, se ignoran los estados preliminares de la neurona, esta suma ponderada representa la activación de la neurona en el tiempo actual. Así, la activación de la neurona $j$ en el tiempo $t$ está dada por: $$a_j(t) = \\sum_{i=1}^n w_i x_i$$\n",
    "4. La salida de la neurona es un proceso \"todo o nada\": si la activación supera un cierto valor umbral $\\theta$ la neurona se activa y transmite un impulso (salida $y=1$), en caso contrario no *dispara*. De esta manera, la salida de la neurons $j$ en el tiempo $t$ está dada por: <br>\n",
    "$$y_j(t) = \n",
    "\\begin{cases} \n",
    "0 \\quad \\text{ if } \\quad a_j(t) < \\theta\\\\ \n",
    "1 \\quad \\text{ if } \\quad a_j(t) \\ge \\theta \n",
    "\\end{cases}\n",
    "$$<br>\n",
    "lo cual puede reescribirse como <br>\n",
    "$$y_j(t) = \n",
    "\\begin{cases} \n",
    "0 \\quad \\text{ if } \\quad \\mathbf{w}_j\\cdot \\mathbf{x}_j(t) < \\theta\\\\ \n",
    "1 \\quad \\text{ if } \\quad \\mathbf{w}_j\\cdot \\mathbf{x}_j(t) \\ge \\theta \n",
    "\\end{cases}\n",
    "$$<br>\n",
    "siendo $\\mathbf{w}_j$ y $\\mathbf{x}_j(t)$ el vector de pesos (constante) de la neurona $j$ y el vector de entrada a la neurona en el tiempo $t$.\n",
    "\n",
    "![](images/neuron2.png)\n",
    "\n",
    "Originalmente, sin un procedimiento adecuado para ajustar los pesos sinápticos, particularmente para neuronas en una red, la neurona de McCulloch-Pitts era apenas una curiosidad y un primer resultado que validaba las posibilidades del uso de metáforas para el desarrollo de sistemas artificiales.\n",
    "\n",
    "### Regla de Hebb\n",
    "\n",
    "La [teoría Hebbiana](http://lcn.epfl.ch/~gerstner/PUBLICATIONS/Gerstner-Plasticity2011.pdf) propuesta por Donald Hebb en 1949 ofrece una explicación del aprendizaje en términos de la adaptación de las neuronas durante su actividad. Esta teoría recibe también el nombre de Regla de Hebb. Hebb la describe de la siguiente manera: \n",
    "\n",
    ">Let us assume that the persistence or repetition of a reverberatory activity (or \"trace\") tends to induce lasting cellular changes that add to its stability. […] When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased.\n",
    "\n",
    "Esta regla puede expresarse, matemáticamente de la siguiente manera: Supongamos que $y_j$ es la salida de la neurona postsináptica $j$ dada la salida $x_{i}$ de la neurona presináptica $i$, entonces, si $\\eta$ es la tasa de aprendizaje, la actualización del peso que la neurona $j$ da a la entrada proveneinte de la neurona $i$ está dada por:\n",
    "\n",
    "$$\\Delta w_{ji}=\\eta x_{i} y_j$$ \n",
    "\n",
    "![](images/neuron3.png)\n",
    "\n",
    "La regla de Hebb, utilizada para actualizar los pesos de la neurona de McCulloch-Pitts, abre la puerta a sistemas neuronales efectivos.\n",
    "\n",
    "\n",
    "### El perceptrón\n",
    "\n",
    "El **[perceptrón](https://en.wikipedia.org/wiki/Perceptron)** es un modelo de neurona obtenido al combinar el modelo de la neurona de McCulloch-Pitts con una extensión de la regla de Hebb. La regla de entrenamiento consta de los siguientes pasos:\n",
    "\n",
    "1. Inicializar los pesos en 0 con valores aleatorios pequeños.\n",
    "2. Para cada vector de entrenamiento $\\mathbf{x}_i$ con valor de salida $y_i$: \n",
    "    1. Calcular el valor de salida  $y^*_i$\n",
    "    2. Actualizar los pesos de acuerdo a la regla de Hebb modificada:<BR>\n",
    "    $$\\Delta \\mathbf{w} = \\eta (y_i - y^*_i) \\mathbf{x}_i$$\n",
    "    \n",
    "\n",
    "A continuación mostramos el algoritmo de entrenaiento del perceptrón. Analizamos el caso de clasificación de los datos de la flor Iris. Dado que el preceptrón tiene sólo dos salidas (0 y 1). sólo podemos clasificar en dos clases por lo que, para este ejercició, utilizaremos sólo los primeros 100 datos del conjunto de datos *Iris Data Set*. Utilizamos los primeros 50 datos (revueltos) para entrenar el perceptrón, imprimimos sólo los casos donde hubo error y, por lo tanto, modificación de los pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector\tTarget\toutput\terror\tw\n",
      "2 \t 0 \t 1 \t -1 \t [-0.057 -0.038 -0.017 -0.003]\n",
      "3 \t 1 \t 0 \t 1 \t [-0.001 -0.011  0.025  0.01 ]\n",
      "5 \t 0 \t 1 \t -1 \t [-0.049 -0.041  0.011  0.007]\n",
      "10 \t 1 \t 0 \t 1 \t [ 0.015 -0.009  0.056  0.022]\n",
      "13 \t 0 \t 1 \t -1 \t [-0.037 -0.043  0.042  0.02 ]\n",
      "14 \t 1 \t 0 \t 1 \t [ 0.019 -0.018  0.081  0.031]\n",
      "15 \t 0 \t 1 \t -1 \t [-0.025 -0.048  0.068  0.029]\n",
      "16 \t 1 \t 0 \t 1 \t [ 0.026 -0.023  0.098  0.04 ]\n",
      "20 \t 0 \t 1 \t -1 \t [-0.025 -0.061  0.082  0.038]\n",
      "Pesos finales:  [-0.025 -0.061  0.082  0.038]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Importar los datos\n",
    "df = pd.read_csv(\"Data sets/Iris Data Set/iris.data\", header = None)\n",
    "\n",
    "# Etiqueta de clase de cada vector ejemplo\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', 0, 1) # convertir las etiquetas de clase a valores de clase 0\n",
    "\n",
    "# Vector de características\n",
    "X = df.iloc[0:100, 0:4].values\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "\n",
    "# Vector de pesos inicial\n",
    "w = np.zeros(X.shape[1])\n",
    "\n",
    "# Datos revueltos\n",
    "data = shuffle(list(zip(X, y)))\n",
    "train_data = data[:50]\n",
    "test_data = data[50:]\n",
    "\n",
    "# Entrenamiento\n",
    "print(\"Vector\\tTarget\\toutput\\terror\\tw\")\n",
    "vector = 1\n",
    "for xi, target in train_data:\n",
    "    activation = np.dot(xi, w)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    w += eta * error * xi\n",
    "    if(error != 0):\n",
    "        print(vector, \"\\t\", target, \"\\t\", output, \"\\t\", error, \"\\t\", w)\n",
    "    vector += 1\n",
    "    \n",
    "print (\"Pesos finales: \", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante notar que los valores de los pesos dependerá de los valores iniciales de los pesos y de la secuencia en que se van presentando los vectores de entrenamiento. En la siguiente tabla se presentan los pesos finales para diversas corridas:\n",
    "\n",
    "Corrida | Pesos finales  \n",
    "----| ----|\n",
    "1   | [-0.018 -0.053  0.074  0.034]\n",
    "2   | [-0.019 -0.067  0.098  0.039]\n",
    "3   | [-0.018 -0.065  0.097  0.038]\n",
    "4   | [-0.022 -0.057  0.085  0.037]\n",
    "5   | [-0.018 -0.065  0.087  0.031]\n",
    "\n",
    "Los valores específicos de los pesos carecen de importancia y lo importante es el desempeño del clasificador.\n",
    "\n",
    "A continuación los utilizamos los siguientes 50 vectores para probar la eficacia del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 vectores mal clasificados de 50 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Prueba\n",
    "errores = 0\n",
    "for xi, target in test_data:\n",
    "    activation = np.dot(xi, w)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(test_data), \n",
    "                                                        errores/len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el experimento, ahora con los datos de diabetes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 vectores mal clasificados de 79 (0.6708860759493671%)\n"
     ]
    }
   ],
   "source": [
    "# Importar los datos\n",
    "df = pd.read_csv(\"Data sets/Pima Indian Data Set/pima-indians-diabetes.data\", \n",
    "                 names = ['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad', 'class'])\n",
    "\n",
    "df.loc[df['pad'] == 0,'pad'] = np.nan\n",
    "df.loc[df['ept'] == 0,'ept'] = np.nan\n",
    "df.loc[df['is2h'] == 0,'is2h'] = np.nan\n",
    "df.loc[df['imc'] == 0,'imc'] = np.nan\n",
    "df = df.dropna()\n",
    "\n",
    "df_pure = df[list(['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad'])]\n",
    "df_class = df[list(['class'])]\n",
    "\n",
    "X_trainPID, X_testPID, y_trainPID, y_testPID = train_test_split(\n",
    "    df_pure.values, df_class.values.ravel(), test_size=.2)\n",
    "\n",
    "# Tasa de aprendizaje\n",
    "eta=0.01\n",
    "\n",
    "# Vector de pesos inicial\n",
    "wPID = np.zeros(X_trainPID.shape[1])\n",
    "\n",
    "# Entrenamiento\n",
    "for xi, target in zip(X_trainPID, y_trainPID):\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    error = target - output\n",
    "    wPID += eta * error * xi\n",
    "    \n",
    "# Prueba\n",
    "errores = 0\n",
    "for xi, target in zip(X_testPID, y_testPID) :\n",
    "    activation = np.dot(xi, wPID)\n",
    "    output = np.where(activation >= 0.0, 1, 0)\n",
    "    if (target != output) :\n",
    "        errores += 1\n",
    "print(\"{} vectores mal clasificados de {} ({}%)\".format(errores, len(y_testPID), \n",
    "                                                        errores/len(y_testPID)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que los resultados para problemas más complejos no es satisfactorio. Este resultado no es realmente una sorpresa dado que una neurona como la planteada es, simplemente, un clasificador lineal. Por otra parte, en términos de la metáfora neuronal, la función de razonamiento no se logra con una sola neurona, sino con un conglomerado, interconectado, de neuronas:\n",
    "\n",
    "![](images/neuron4.jpg)\n",
    "\n",
    "Una arquitectura que se acerca a la metáfora neuronal es la siguiente:\n",
    "\n",
    "![](images/full_topo.png)\n",
    "\n",
    "En este caso, se trata de una *[red neuronal de impulsos](https://en.wikipedia.org/wiki/Spiking_neural_network)* *[conectada completamente](https://en.wikipedia.org/wiki/Network_topology#Mesh)*. \n",
    "\n",
    "Sin embargo, una opción mucho más común, es el **multiperceptrón multicapa**:\n",
    "\n",
    "![](images/neuron5.png)\n",
    "\n",
    "El multiperceptrón multicapa es una malla de neuronas organizadas en una sola dirección, desde una primera *capa de entrada* que recibe todos los estímulos a la entrada de la red (siguiendo la metáfora, estos estímulos pueden ser las señales de las neuronas sensoras) y transfieren estas señales a un conjunto de capas, avanzando hasta la *capa de salida*. La capa de salida arroja la o las salidas de la red neuronal (por ejemplo, las señales hacia las neuronas motoras).\n",
    "\n",
    "Las capas intermedias (entre la capa de entrada y la capa de salida) se denominan *capas ocultas*. Este nombre se debe a que en estas capas desconocemos las entradas y las salidas de cada neurona. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
