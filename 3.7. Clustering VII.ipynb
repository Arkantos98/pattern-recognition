{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%; overflow:hidden; background-color:#F1F1E6; padding: 10px; border-style: outset; color:#17469e\">\n",
    "    <div style=\"width: 80%; float: left;\">\n",
    "    <h2 align=\"center\">Universidad de Sonora</h2>\n",
    "    <hr style=\"border-width: 3px; border-color:#17469e\">\n",
    "          <h1>Reconocimiento de patrones: Preparación de los datos</h1>          \n",
    "          <h4>Ramón Soto C. <a href=\"mailto:rsotoc@moviquest.com/\">(rsotoc@moviquest.com)</a></h4>\n",
    "    </div>\n",
    "    <div style=\"float: right;\">\n",
    "    <img src=\"images/escudo_unison.png\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de estudio: [*Stack Overflow 2018 Developer Survey*](https://www.kaggle.com/stackoverflow/stack-overflow-2018-developer-survey)\n",
    "\n",
    "Como caso de estudio principal en el presente curso hemos seleccionado la encuesta de desarrolladores 2018 de *Stack Overflow* disponible en [Kaggle](https://www.kaggle.com). En este esta etapa realizaremos el análisis de agrupamientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modelado - ISODATA\n",
    "\n",
    "<div style=\"margin-top: 6px; border: 1px solid #cfcfcf; padding: 8px 12px; border-radius:2px; background-color:#f7f7f7; \">\n",
    "... ahora utilizamos la técnica ISODATA para identificar prototipos de clases. <br>Inicializamos el contexto y cargamos los datos:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reconocimiento de patrones: ISODATA\n",
    "\"\"\"\n",
    "\n",
    "#from scipy.spatial.distance import squareform\n",
    "\n",
    "# Inicializar el ambiente\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "#import math\n",
    "import random\n",
    "#import time\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "#from scipy.spatial.distance import euclidean, pdist, squareform\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True) # Cortar la impresión de decimales a 1\n",
    "pd.set_option('display.max_columns', 130)\n",
    "pd.set_option('max_colwidth', 80)\n",
    "\n",
    "LARGER_DISTANCE = sys.maxsize\n",
    "TALK = True # TALK = True, imprime resultados parciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"Data sets/Stack Overflow Survey/\"\n",
    "\n",
    "# Recuperar encabezados de columnas en orden original\n",
    "with open(path + 'survey_results_public_transformed.headers', 'rb') as file:  \n",
    "    headers = pickle.load(file)\n",
    "\n",
    "# Recuperar diccionarios... sólo por si se requieren\n",
    "with open(path + 'survey_results_public_transformed.dicts', 'rb') as file:  \n",
    "    dict_of_dicts = pickle.load(file)\n",
    "\n",
    "with open(path + 'survey_results_public_transformed.json') as f:\n",
    "    dict_json = json.load(f)\n",
    "df = pd.DataFrame.from_dict(dict_json)\n",
    "#df = df.sample(n=1000).reset_index(drop=True)\n",
    "\n",
    "# Reordenar las columnas de acuerdo al orden original\n",
    "df = df.reindex(headers, axis=1)\n",
    "\n",
    "DATA_LEN = df.shape[0]\n",
    "\n",
    "# Agregar una columna \"cluster\" inicializada a null \n",
    "df[\"Cluster\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_str = ['Hobby', 'OpenSource', 'Country', 'Student', 'Employment', 'FormalEducation', \n",
    "         'UndergradMajor', 'CompanySize', 'YearsCoding', 'YearsCodingProf', 'UpdateCV', \n",
    "         'JobSatisfaction', 'CareerSatisfaction', 'HopeFiveYears', 'JobSearchStatus', \n",
    "         'LastNewJob', 'TimeFullyProductive', 'AgreeDisagree1', 'AgreeDisagree2', \n",
    "         'AgreeDisagree3', 'OperatingSystem', 'NumberMonitors', 'CheckInCode', 'AdBlocker', \n",
    "         'AdBlockerDisable', 'AdsAgreeDisagree1', 'AdsAgreeDisagree2', 'AdsAgreeDisagree3', \n",
    "         'AIDangerous', 'AIInteresting', 'AIResponsible', 'AIFuture', 'EthicsChoice', \n",
    "         'EthicsReport', 'EthicsResponsible', 'EthicalImplications', 'HoursComputer', \n",
    "         'StackOverflowRecommend', 'StackOverflowVisit', 'StackOverflowHasAccount', \n",
    "         'StackOverflowParticipate', 'StackOverflowJobs', 'StackOverflowDevStory', \n",
    "         'StackOverflowJobsRecommend', 'StackOverflowConsiderMember', 'HypotheticalTools1', \n",
    "         'HypotheticalTools2', 'HypotheticalTools3', 'HypotheticalTools4', 'WakeTime', \n",
    "         'HypotheticalTools5', 'HoursOutside', 'SkipMeals', 'Exercise', 'EducationParents', \n",
    "         'Age', 'Dependents', 'SurveyTooLong', 'SurveyEasy']\n",
    "var_list = ['DevType', 'CommunicationTools', 'EducationTypes', 'SelfTaughtTypes', \n",
    "         'HackathonReasons', 'LanguageDesireNextYear', 'DatabaseWorkedWith', \n",
    "         'DatabaseDesireNextYear', 'PlatformWorkedWith', 'PlatformDesireNextYear', \n",
    "         'FrameworkWorkedWith', 'FrameworkDesireNextYear', 'IDE', 'Methodology', \n",
    "         'VersionControl', 'AdBlockerReasons', 'AdsActions', 'ErgonomicDevices', \n",
    "         'RaceEthnicity', 'LanguageWorkedWith']\n",
    "var_ranks = ['AssessJob', 'AssessBenefits', 'JobContactPriorities', 'JobEmailPriorities', \n",
    "             'AdsPriorities']\n",
    "var_float = 'ConvertedSalary'\n",
    "\n",
    "def distance_qual(x, y):\n",
    "    # Número de variables; si var_float es array, modificar \"+ 1\" por \"+ len(var_float)\"\n",
    "    numvars = len(var_str) + len(var_list) + len(var_ranks) + 1\n",
    "    \n",
    "    distancia = abs(x.ConvertedSalary - y.ConvertedSalary)\n",
    "    if pd.isnull(distancia):\n",
    "        distancia = 0\n",
    "        numvars -= 1\n",
    "        \n",
    "    for col in var_str:\n",
    "        if x[col] != y[col]:\n",
    "            distancia += 1\n",
    "        \n",
    "    for col in var_list:\n",
    "        num_vars = len(x[col]) + len(y[col])\n",
    "        d = 0\n",
    "        if num_vars > 0:\n",
    "            d = (2*len(set(x[col] + y[col])) - num_vars) / num_vars\n",
    "        distancia += d\n",
    "\n",
    "    for col in var_ranks:\n",
    "        d = 0\n",
    "        max_vars = max(len(x[col]), len(y[col]))\n",
    "        if len(x[col]) != 0 and len(y[col]) != 0:\n",
    "            for v in range(len(x[col])):\n",
    "                if x[col][v] != y[col][v]:\n",
    "                    d += 1\n",
    "        else:\n",
    "            d += max_vars\n",
    "        \n",
    "        if d != 0:\n",
    "            d /= max_vars\n",
    "        distancia += d\n",
    "\n",
    "    return distancia / numvars\n",
    "    \n",
    "def decode(dataframe):\n",
    "    new_df = dataframe.copy(deep=True)\n",
    "    \n",
    "    for col in var_str:\n",
    "        if col in list(dataframe) and col in dict_of_dicts:\n",
    "            for index, row in dataframe.iterrows():\n",
    "                value = dict_of_dicts[col][row[col]]\n",
    "                new_df.at[clusters.index[index], col] = value\n",
    "                \n",
    "    for index, row in dataframe.iterrows():\n",
    "        new_df.at[clusters.index[index], 'ConvertedSalary'] = row['ConvertedSalary'] * 200000\n",
    "    \n",
    "    for col in var_list:\n",
    "        if col in list(dataframe):\n",
    "            for index, row in dataframe.iterrows():\n",
    "                values_list = row[col].copy()\n",
    "                for i in range(len(values_list)):\n",
    "                    values_list[i] = dict_of_dicts[col][values_list[i]]\n",
    "                new_df.at[clusters.index[index], col] = values_list\n",
    "                \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin-top: 6px; border: 1px solid #cfcfcf; padding: 8px 12px; border-radius:2px; background-color:#f7f7f7; \">\n",
    "A continuación ejecutamos el algoritmo ISODATA:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Definir los valores de $k_{init}, n_{min}, I_{max}, \\sigma_{max}, L_{min}$ y $P_{max}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K_INIT = 7\n",
    "N_MIN = 100\n",
    "I_MAX = 10\n",
    "S_MAX = .8 # La desviación estándar está normalizada\n",
    "D_MAX = 3 # El cluster sólo se divide cuando hay al menos estas variables con s>S_MAX\n",
    "\n",
    "L_MIN = .25 # Las distancia están normalizadas\n",
    "P_MAX = 2\n",
    "\n",
    "NUM_CLUSTERS = K_INIT # valor de k\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Seleccionar de manera arbitraria *k* puntos en el espacio de características como centros iniciales de los clusters (centroides o centros de masa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inicializar los centroides\n",
    "centroids = df.sample(n=NUM_CLUSTERS).reset_index(drop=True)\n",
    "display(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Asignar cada punto del conjunto de datos al cluster donde la distancia del punto al centroide es menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def update_clusters():\n",
    "    global NUM_CLUSTERS, centroids\n",
    "    changed = False\n",
    "    cluster_col_index = df.shape[1] - 1\n",
    "    \n",
    "    if TALK :\n",
    "        print(\"Actualizando clusters\")\n",
    "    for index, row in df.iterrows():\n",
    "        dists = []\n",
    "        for i, r in centroids.iterrows():\n",
    "            dists.append(distance_qual(row, r))\n",
    "        cluster = np.argmin(dists)\n",
    "        \n",
    "        # Si hay cambio, realizarlo y levantar la bandera 'changed'\n",
    "        if(pd.isnull(row['Cluster']) or row['Cluster'] != cluster):\n",
    "            df.iloc[index, cluster_col_index] = cluster\n",
    "            changed = True\n",
    "            \n",
    "    # Contabilizar los elementos en cada cluster   \n",
    "    to_eliminate = []\n",
    "    for i in range(NUM_CLUSTERS):\n",
    "        members = df[df[\"Cluster\"]==i].count()[\"Cluster\"]\n",
    "        if members < N_MIN:\n",
    "            to_eliminate.append(i)\n",
    "        if (TALK) : \n",
    "            print(\"El cluster \", i, \" incluye \", members, \"miembros.\")\n",
    "    if (TALK) : \n",
    "        print()\n",
    "\n",
    "    if len(to_eliminate) > 0:\n",
    "        if (TALK) : \n",
    "            print(\"Clusters a eliminar:\", to_eliminate)\n",
    "        \n",
    "        # Eliminar los centroides seleccionados\n",
    "        centroids.drop(to_eliminate, inplace=True)    \n",
    "        centroids = centroids.reset_index(drop=True)\n",
    "        \n",
    "        # Reetiquetar los registros afectados\n",
    "        eliminated = 0\n",
    "        for i in to_eliminate:\n",
    "            i_e = i - eliminated\n",
    "            # Reetiquetar como Null los registros en cada cluster eliminado\n",
    "            df.loc[df.Cluster == i_e, 'Cluster'] = np.nan\n",
    "            # Recorrer las etiquetas para coincidir con los nuevos índices\n",
    "            for cj in range(i_e + 1, NUM_CLUSTERS):\n",
    "                df.loc[df.Cluster == cj, 'Cluster'] = cj - 1\n",
    "            # Actualizar el número actual de centroides\n",
    "            NUM_CLUSTERS -= 1\n",
    "            eliminated += 1\n",
    "            \n",
    "#        if (TALK) : \n",
    "#            for i in range(NUM_CLUSTERS):\n",
    "#                members = df[df[\"Cluster\"]==i].count()[\"Cluster\"]\n",
    "#                print(\"El cluster \", i, \" incluye \", members, \"miembros.\")\n",
    "\n",
    "        changed = True\n",
    "        \n",
    "    if changed:\n",
    "        if TALK : \n",
    "            faltantes = df[pd.isnull(df[\"Cluster\"])].shape[0]\n",
    "            if faltantes > 0:\n",
    "                print(\"Faltan por clasificar\", faltantes, \"miembros.\\n\")\n",
    "            else :\n",
    "                print()\n",
    "                \n",
    "        # Reclasificar los registros afectados\n",
    "        for index, row in df[pd.isnull(df[\"Cluster\"])].iterrows():\n",
    "            dists = []\n",
    "            for i, r in centroids.iterrows():\n",
    "                dists.append(distance_qual(row, r))\n",
    "            df.iloc[index, cluster_col_index] = np.argmin(dists)\n",
    "                \n",
    "        # Contabilizar los elementos en cada cluster   \n",
    "        if TALK : \n",
    "            for i in range(NUM_CLUSTERS):\n",
    "                members = df[df[\"Cluster\"]==i].count()[\"Cluster\"]\n",
    "                print(\"El cluster \", i, \" incluye \", members, \"miembros.\")\n",
    "            print()\n",
    "        \n",
    "    return changed\n",
    "\n",
    "# --------------------------\n",
    "# Actualizar los clusters\n",
    "KEEP_WALKING = update_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Calcular los centroides a partir de los puntos en cada cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_centroids():\n",
    "    global centroids\n",
    "    \n",
    "    for cl_j in range(NUM_CLUSTERS):        \n",
    "        # Seleccionar registros en el cluster cl_j\n",
    "        df_clusterj = df[df[\"Cluster\"] == cl_j]\n",
    "        \n",
    "        centroids.loc[centroids.index[cl_j]] = get_centroide(df_clusterj).loc[0]        \n",
    "    return\n",
    "\n",
    "def get_centroide(data):\n",
    "    # Copiar estructura de la tabla\n",
    "    df2 = pd.DataFrame(data=None, columns=data.columns)\n",
    "    #df2.append(pd.Series([np.nan]), ignore_index = True)\n",
    "\n",
    "    col = 'ConvertedSalary'\n",
    "    df2.at[0, col] = data[col].mean()\n",
    "\n",
    "    # Moda en las columnas 'simples' (en var_str)\n",
    "    mode = data[var_str].mode()\n",
    "    for col in mode:\n",
    "        df2.at[0, col] = mode[col].values[0]\n",
    "\n",
    "    # Moda en las columnas con listas de longitud variable (en var_list)\n",
    "    for col in var_list:\n",
    "        mean_len = 0\n",
    "        vars_list = []\n",
    "        for index, row in data.iterrows():\n",
    "            mean_len += len(row[col])\n",
    "            vars_list = vars_list + row[col]\n",
    "        mean_len /= data.shape[0]\n",
    "        counter = Counter(vars_list)\n",
    "        mean_list = []\n",
    "        for v in counter.most_common(round(mean_len + 0.5)):\n",
    "            mean_list.append(v[0])\n",
    "        df2.at[0, col] = mean_list\n",
    "\n",
    "\n",
    "    # Moda en las columnas con listas de longitud fija (en var_ranks)\n",
    "    ranges = [11, 12, 6, 8, 8]\n",
    "    # Para cada variable en var_list, obtener el número de componentes en el vector\n",
    "    # y el nombre de la columna\n",
    "    for i, col in zip(range(len(ranges)), var_ranks):\n",
    "        # Inicializar una matriz (lista de listas, en realidad), con tantos renglones como \n",
    "        # componentes tiene el vector de la variable. Cada renglón tiene todos los valores \n",
    "        # utilizados en cada posición del vector\n",
    "        vars = []\n",
    "        for j in range(ranges[i] - 1):\n",
    "            vars.append([])\n",
    "\n",
    "        # Recorrer todos los elementos actualmente en el cluster para rellenar la matriz\n",
    "        for index, row in data.iterrows():\n",
    "            # Si el vector de la variable no está vacío...\n",
    "            if len(row[col]) > 0:\n",
    "                # Para cada componente en el vector...\n",
    "                for j in range(len(row[col])):\n",
    "                    # Si no es 0\n",
    "                    if row[col][j] != '0':\n",
    "                        # Agregarla al renglón actual en la matriz\n",
    "                        vars[j].append(row[col][j])\n",
    "\n",
    "        \n",
    "        # Contabilizar ocurrencias de cada componente. Crear una matriz con el orden para\n",
    "        # cada componente como renglones\n",
    "        most_commons = []\n",
    "        for j in range(ranges[i] - 1):\n",
    "            counter = Counter(vars[j])\n",
    "            #most_commons.append(counter.most_common(ranges[i] - 1))\n",
    "            most_commons.append(counter.most_common())\n",
    "\n",
    "        # Inicializar vector. Se escoge el valor más popular en la primera componente\n",
    "        if len(most_commons) > 0 and len(most_commons[0]) > 0:\n",
    "            vars_list = [most_commons[0][0][0]]\n",
    "            # Para cada componente a partir de la segunda...\n",
    "            for j in range(1, ranges[i] - 1):\n",
    "                # Buscar la componente más común...\n",
    "                for c in most_commons[j]:\n",
    "                    # Siempre y cuando no esté utilizada...\n",
    "                    if c[0] not in vars_list[:j]:\n",
    "                        # Agregarla al vector y...\n",
    "                        vars_list.append(c[0])\n",
    "                        # Dejar de buscar.\n",
    "                        break\n",
    "\n",
    "        if len(vars_list) < ranges[i] - 1:\n",
    "            for i in set(range(1, ranges[i])):\n",
    "                if str(i) not in vars_list:\n",
    "                    vars_list.append(str(i))\n",
    "        df2.at[0, col] = vars_list\n",
    "\n",
    "    return df2\n",
    "\n",
    "# --------------------------\n",
    "# Actualizar los centroides\n",
    "update_centroids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deltas = []\n",
    "delta = 0\n",
    "def update_deltas():\n",
    "    global deltas, delta, centroids\n",
    "    deltas = [0] * NUM_CLUSTERS\n",
    "    N = 0\n",
    "    for j, rc in centroids.iterrows():\n",
    "        n = 0\n",
    "        for i, row in df[df[\"Cluster\"]==j].iterrows():\n",
    "            deltas[j] += distance_qual(row, rc)\n",
    "            n += 1\n",
    "        delta += deltas[j]\n",
    "        deltas[j] /= n\n",
    "        N += n\n",
    "    delta /= N\n",
    "    \n",
    "    if TALK : \n",
    "        print(\"Las distancias medias en cada cluster son:\\n\", deltas)   \n",
    "        print(\"\\nLa distancia media promedio es:\", delta)   \n",
    "        \n",
    "    return\n",
    "\n",
    "update_deltas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def std_dev():\n",
    "    # Inicializar vector de desviaciones estándar... los valores actuales son inserbibles\n",
    "    std_vectors = centroids.copy()\n",
    "    \n",
    "    for c in range(NUM_CLUSTERS) :\n",
    "        df_c = df[(df[\"Cluster\"]==c)]\n",
    "        \n",
    "        # Para cada variable numérica...\n",
    "        df_cj = df_c[pd.notnull(df_c['ConvertedSalary'])]\n",
    "\n",
    "        s = math.sqrt(sum(abs(df_cj[\"ConvertedSalary\"] - \n",
    "                              centroids.iloc[c][\"ConvertedSalary\"])) / (df_cj.shape[0] - 1))\n",
    "        std_vectors.loc[c, \"ConvertedSalary\"] = s\n",
    "        \n",
    "        for col in var_str:\n",
    "            diff = sum(df_cj[col] != centroids.iloc[c][col])\n",
    "            s = math.sqrt(diff / (df_cj.shape[0] - 1))\n",
    "            std_vectors.loc[c, col] = s\n",
    "        \n",
    "        for col in var_list:\n",
    "            y = centroids.iloc[c][col]\n",
    "            diff = 0\n",
    "            for i, row in df_cj.iterrows():\n",
    "                x = row[col]\n",
    "                num_vars = len(x) + len(y)\n",
    "                if num_vars > 0:\n",
    "                    diff += (2*len(set(x + y)) - num_vars) / num_vars\n",
    "            s = math.sqrt(diff / (df_cj.shape[0] - 1))\n",
    "            std_vectors.loc[c, col] = s\n",
    "        \n",
    "        for col in var_ranks:\n",
    "            y = centroids.iloc[c][col]\n",
    "            for i, row in df_cj.iterrows():\n",
    "                diff = 0\n",
    "                x = row[col]\n",
    "                max_vars = max(len(x), len(y))\n",
    "                if len(x) != 0 and len(y) != 0:\n",
    "                    for v in range(len(x)):\n",
    "                        if x[v] != y[v]:\n",
    "                            diff += 1\n",
    "                else:\n",
    "                    diff += max_vars\n",
    "\n",
    "                if diff != 0:\n",
    "                    diff /= max_vars\n",
    "            s = math.sqrt(diff / (df_cj.shape[0] - 1))\n",
    "            std_vectors.loc[c, col] = s\n",
    "         \n",
    "    return std_vectors\n",
    "\n",
    "display(std_dev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def divide_clusters():\n",
    "    global NUM_CLUSTERS, centroids\n",
    "\n",
    "    if TALK :\n",
    "        display(centroids)\n",
    "    \n",
    "    # Cálculo de desviaciones estandar\n",
    "    sigma_vect = std_dev()   \n",
    "    if TALK :\n",
    "        display(sigma_vect)\n",
    "    \n",
    "    candidates = []\n",
    "    for c, s_row in sigma_vect.iterrows():\n",
    "        for col in s_row:\n",
    "            if col > S_MAX :\n",
    "                candidates.append(c)\n",
    "                break # Ya encontramos un atributo con sigma elevada \n",
    "\n",
    "    if TALK :\n",
    "        print(\"Posibles clusters a dividir:\", candidates)\n",
    "    \n",
    "    divided = False\n",
    "    to_eliminate = []\n",
    "    for c in candidates:\n",
    "        members = df[df[\"Cluster\"]==c].count()[\"Cluster\"]\n",
    "        cond = NUM_CLUSTERS < K_INIT/2 or (deltas[c] > delta and members > 2 * N_MIN)\n",
    "        if cond: \n",
    "            d = 0\n",
    "            # Obtener dos puntos \"suficientemente separados\", no es el óptimo, \n",
    "            # pero son buenos candidatos a buen costo\n",
    "            count = 0\n",
    "            while d < deltas[c] and count < 5000:\n",
    "                s1 = df[df[\"Cluster\"]==c].sample(n=2)\n",
    "                d = distance_qual(s1.iloc[0], s1.iloc[1])\n",
    "                count += 1\n",
    "            if count < 5000:\n",
    "                to_eliminate.append(c)\n",
    "                centroids = centroids.append(s1)\n",
    "                NUM_CLUSTERS += 1\n",
    "            \n",
    "    if len(to_eliminate) > 0 :\n",
    "        if TALK : \n",
    "            print(\"Clusters a eliminar:\", to_eliminate)\n",
    "            print(\"\")\n",
    "        centroids.drop(to_eliminate, inplace=True)\n",
    "        centroids = centroids.reset_index(drop=True)\n",
    "        update_clusters()\n",
    "        update_centroids()\n",
    "        if TALK : \n",
    "            display(centroids)\n",
    "            print(\"\")\n",
    "            \n",
    "    return \n",
    "\n",
    "#divide_clusters()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mix_clusters():\n",
    "    global centroids, NUM_CLUSTERS\n",
    "    \n",
    "    # Matriz triangular superior de distancias entre centroides\n",
    "    dist_lists = []\n",
    "    for i, rc_i in centroids.iterrows():\n",
    "        dist_lists.append([])\n",
    "        for j, rc_j in centroids.iterrows():\n",
    "            if j <= i:\n",
    "                dist_lists[i].append(LARGER_DISTANCE)\n",
    "            else:\n",
    "                dist_lists[i].append(distance_qual(rc_i, rc_j))\n",
    "    dist_matrix = np.array(dist_lists)\n",
    "    \n",
    "    to_eliminate = []\n",
    "    # to_eliminate contendrá la mitad de los clusters unidos...\n",
    "    while (dist_matrix.min() < LARGER_DISTANCE and len(to_eliminate) < P_MAX/2) :\n",
    "        dist_min = dist_matrix.min()\n",
    "        idx = (dist_matrix==dist_min).argmax()\n",
    "        z1 = idx // len(centroids)\n",
    "        z2 = idx % len(centroids)\n",
    "        \n",
    "        if dist_min < L_MIN:\n",
    "            if TALK:\n",
    "                print(\"Unificando clusters {} y {}\".format(z1, z2))\n",
    "                for i in range(NUM_CLUSTERS):\n",
    "                    members = df[df[\"Cluster\"]==i].count()[\"Cluster\"]\n",
    "                    print(\"El cluster \", i, \" incluye \", members, \"miembros.\")\n",
    "                print()\n",
    "\n",
    "            # Modificar z1 para contener el centroide entre z1 y z2\n",
    "            centroids.iloc[z1] = get_centroide(centroids.iloc[[z1, z2]]).loc[0]\n",
    "            # Marcar puntos en z1 y z2 para reclasificar\n",
    "            df.loc[df.Cluster == z1, 'Cluster'] = np.nan\n",
    "            df.loc[df.Cluster == z2, 'Cluster'] = np.nan\n",
    "            \n",
    "            # Marcar z2 para eliminación\n",
    "            to_eliminate.append(z2)\n",
    "        \n",
    "        dist_matrix[z1][z2] = LARGER_DISTANCE\n",
    "        \n",
    "    if len(to_eliminate) > 0:\n",
    "        centroids.drop(to_eliminate, inplace=True)\n",
    "        centroids = centroids.reset_index(drop=True)\n",
    "        \n",
    "        # Reetiquetar los registros afectados\n",
    "        eliminated = 0\n",
    "        for i in to_eliminate:\n",
    "            i_e = i - eliminated\n",
    "            # Recorrer las etiquetas para coincidir con los nuevos índices\n",
    "            for cj in range(i_e + 1, NUM_CLUSTERS):\n",
    "                df.loc[df.Cluster == cj, 'Cluster'] = cj - 1\n",
    "            # Actualizar el número actual de centroides\n",
    "            NUM_CLUSTERS -= 1\n",
    "            eliminated += 1\n",
    "            \n",
    "        cluster_col_index = df.shape[1] - 1\n",
    "        for index, row in df[pd.isnull(df[\"Cluster\"])].iterrows():\n",
    "            dists = []\n",
    "            for i, r in centroids.iterrows():\n",
    "                dists.append(distance_qual(row, r))\n",
    "            df.iloc[index, cluster_col_index] = np.argmin(dists)\n",
    "        update_centroids()\n",
    "            \n",
    "        if (TALK) : \n",
    "            # Contabilizar los elementos en cada cluster   \n",
    "            for i in range(NUM_CLUSTERS):\n",
    "                members = df[df[\"Cluster\"]==i].count()[\"Cluster\"]\n",
    "                print(\"El cluster \", i, \" incluye \", members, \"miembros\")\n",
    "            print()\n",
    "\n",
    "    return\n",
    "\n",
    "#mix_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reproducido aquí para facilitar la ejecución\n",
    "#iteration +=1 #usar si se está probando dividir/unir demostrativo\n",
    "\n",
    "I_MAX_INT = 5 # Iteraciones permitidas en cada ciclo k-means\n",
    "\n",
    "while iteration < I_MAX:\n",
    "    if NUM_CLUSTERS <= K_INIT / 2 :\n",
    "        update_deltas()\n",
    "        divide_clusters()\n",
    "    elif (iteration % 2 == 0 or NUM_CLUSTERS > 2 * K_INIT) :\n",
    "        mix_clusters()\n",
    "        \n",
    "    step = 0\n",
    "    while KEEP_WALKING and step < I_MAX_INT :\n",
    "        KEEP_WALKING = update_clusters()\n",
    "        update_centroids()\n",
    "            \n",
    "    iteration += 1\n",
    "    \n",
    "if TALK : \n",
    "    print (\"No más cambios.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
